{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElM1_WkJMx97"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rges-pit/minicourses/blob/main/chapter5/Chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "*To open this notebook directly in Colab, you need to be logged into your Google account.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WqTTenRT-M"
      },
      "source": [
        "# <font face=\"Helvetica\" size=\"7\">Chapter 5: Mini Data Challenge</font>  \n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "<i> Authors: Amber Malpas, Katarzyna Kruszyńska, Somayeh Khakpash, Ali Crisp </i>\n",
        "\n",
        "<br>\n",
        "\n",
        "If you would like an introduction to python notebooks, please read this tutorial: https://medium.com/codingthesmartway-com-blog/getting-started-with-jupyter-notebook-for-python-4e7082bd5d46\n",
        "\n",
        "Please note, you must **save this notebook in a space owned by you** (a GitHub repo, gist, to Google Drive, or locally) if you want to come back to it later without losing your progress. You can edit and run this notebook on Colab, but it **will not auto save** for you.\n",
        "\n",
        "If you choose to use lcoal resources your notebook will use your local packages, so you should follow install a virtual environment with the following packages. Run the cell below to create a downloadable `.yml` file, to automate the package install process (provided you are using anaconda)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2vVOVzDYO_T"
      },
      "outputs": [],
      "source": [
        "yaml = '''name: roman_fit\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - numpy\n",
        "  - matplotlib\n",
        "  - pandas\n",
        "  - scipy\n",
        "  - jupyter\n",
        "  - ipython\n",
        "  - astropy\n",
        "  - beautifulsoup4\n",
        "  - lxml              # required parser for bs4\n",
        "  - pip\n",
        "  - pip:\n",
        "      - pathos\n",
        "      - MulensModel'''\n",
        "\n",
        "# save the yaml\n",
        "with open('environment.yml', 'w') as f:\n",
        "    f.write(yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "691cvbFVXWGI"
      },
      "source": [
        "Click the folder button on the side bar to open the file explorer. The file `environment.yml` should be in there now. Just click the triple dots on the side and then `Download` to download the `.yml` file.\n",
        "\n",
        "```bash\n",
        "conda env create -f environment.yml\n",
        "```\n",
        "\n",
        "Running the above line in a terminal (Anaconda Prompt on Windows) will create a virtual conda environment called `minicourse`, which has the required packages installed.\n",
        "\n",
        "You can activate the environment with:\n",
        "\n",
        "```bash\n",
        "conda activate minicourse\n",
        "```\n",
        "\n",
        "From here you have two options\n",
        "\n",
        "1. You can open the notebook running\n",
        "```bash\n",
        "jupyter notebook\n",
        "```\n",
        "from a parent folder to your locally saved version of this notebook and navigating to the notebook in your browser. You may need to select `minicourse` as your kernel before running the notebook.\n",
        "\n",
        "2. Alternatly, you can create a local \"Runtime\" and for your Colab notebook by following [these instructions](https://www.google.com/url?q=https%3A%2F%2Fresearch.google.com%2Fcolaboratory%2Flocal-runtimes.html).\n",
        "```bash\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --no-browser\n",
        "```\n",
        "\n",
        "  ⚠️ We don't generally recommend that you do this with notebooks that you didn't write as it gives them access to your local machine.\n",
        "\n",
        "<!--\n",
        "## <font face=\"Helvetica\" size=\"6\"> Dev Notes </font>\n",
        "<hr style=\"border: 1.5pt solid #fc3d21; width: 100%; margin-top: -10px;\">\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqQ1c61HXfcS"
      },
      "source": [
        "<!-- ### <font face=\"Helvetica\" size=\"5\"> Overview </font>\n",
        "\n",
        "The current plan is to give the students a miniature \"season\" of data to fit (as in a couple hundred lightcurves), and an example of how to fit a whole season in a notebook. This example will be using pyLiMA for the model fitting. If we are using Mulens Model we will probably just have to pretend that the observatory is on Earth.\n",
        "\n",
        "The data will be the same set that the C4 data come from.\n",
        "\n",
        "The notebook should probably include a pyLiMASS example.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> Links to Content </font>\n",
        "\n",
        "https://lsu.app.box.com/s/qx440yp9ekzrhaevtfu7ksnfgh2jhc29\n",
        "\n",
        "https://drive.google.com/drive/folders/1jNKztBVCQHZTs5iPVJDFTy1btLGn15s3?usp=drive_link\n",
        "\n",
        "We need a single lens set. I might have to generate it. Or ask Ali to.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> Ideas </font>\n",
        "\n",
        "* Mass measurement?\n",
        "* Astrometric microlensing?\n",
        "* False positives in the data?\n",
        "* Demographics?\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> Note </font>\n",
        "\n",
        "> I'm staring by just doing a bunch of coding examples. I'll remove pieces and turn them in to exercises after.\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raxTCRLV8zJ_"
      },
      "source": [
        "<!--\n",
        "> ### Rough Plan\n",
        ">\n",
        ">**Chapter 5: Mini Data Challenge – Bulk Microlensing Analysis**\n",
        ">\n",
        "> 1. **Introduction**  \n",
        ">    - Briefly explain the motivation for bulk data analysis in the Roman era: processing hundreds or thousands of events without manually tweaking each one.  \n",
        ">    - Emphasize that while Chapter 4 focused on hands-on analysis for individual events, Chapter 5 is about scaling up—addressing the pitfalls and nuances encountered when working with large datasets.\n",
        ">\n",
        "> 2. **Single Event Fit**\n",
        ">    - Recap of Chapter 4\n",
        ">    -  **Special Topics** (Optional Sections):\n",
        ">      - **Astrometry:**  \n",
        ">        Outline how astrometric microlensing can provide additional constraints, noting that this is an extra module for those interested.  \n",
        ">      - **Mass Estimates:**  \n",
        ">        Provide a brief discussion (and possibly code snippets) on deriving mass estimates from parallax and finite source effects, with the understanding that full implementation is an advanced topic.\n",
        ">\n",
        "> 3. **Small subset**\n",
        ">    - Set up the framework for fitting the entire season as an exercise.   \n",
        ">    - Test in out on a small subset of the data\n",
        ">    - **Parallelization for Computational Efficiency**  \n",
        ">      - Describe the challenges of running heavy computations in a Jupyter Notebook, especially the \"frozen function\" issue when parallelizing.  \n",
        ">      - Demonstrate a small-scale example of in-notebook parallelization on the same small subset\n",
        ">      - Explain that kernel restarts may be necessary when modifying functions.\n",
        ">\n",
        ">    - **Future Directions and Advanced Topics**  \n",
        ">      - Briefly outline how an automated anomaly detection method (e.g., using a windowed reduced chi-squared metric) could help flag problematic events for further analysis.  \n",
        ">      - Provide links or references (e.g., RTModel documentation) for students who want to delve deeper into automation.\n",
        ">\n",
        "> 4. **Full Season**\n",
        ">    - Fit a mini season's worth of microlensing events\n",
        ">    - **Initial Fitting and Residual Inspection**\n",
        ">      - **Parameter Degeneracies, Priors and Bounds:**  (This should be mostly covered in Chapter 4)\n",
        ">\n",
        ">        Discuss the degeneracies (e.g., u_0, t_E vs. F_B) and how to set sensible priors (e.g. log rho) or bounds(e.g. u_0>0).\n",
        ">      - **Inintial Parameter Estimates**\n",
        ">        Run a mini-season-wide basic fit (using a basic PSPL fit) on all events. This model is covered in Chapter 4, we are just smashing through it to get our parameter estimates for a higher order fit.\n",
        ">      - Explain how deviations in the residuals can indicate the need for a more complex (binary) model.  \n",
        ">    - **False Positives**\n",
        ">      - **Manual Inspection:**  \n",
        ">        Show a subset of around 10 lightcurves for detailed manual inspection.\n",
        ">      - Somayeh: include a discussion on false positives and how to recognize them and, ideally, include them in the dataset.\n",
        ">    - **Higher Order Effects**\n",
        ">      - turn on the higher order effects needed for mass estimates and let it go.\n",
        ">\n",
        ">    - **Mass**\n",
        ">      Leave this as a homework exercise\n",
        ">\n",
        "> 5. **Advanced Fitting Techniques for Complex Events**  (Move this to Chpater 4 and leave it as information, not an exercise)\n",
        ">    - **Robust Sampling Methods:**  \n",
        ">      Introduce why gradient descent methods fail for binary events, and present **emcee** (or another MCMC sampler) as a better alternative for exploring spastic likelihood spaces.  \n",
        ">    - **Priors**  \n",
        ">    - **Binary-Lens Degeneracies:**  \n",
        "     Mention that a broad grid search is often employed to explore the parameter space thoroughly, though we won’t implement this in full here.\n",
        "\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac9yMcR0AEBb"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> 1. Introduction </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "Welcome to **Chapter 5: Mini Data Challenge - Bulk Microlensing Analysis**.\n",
        "\n",
        "In this chapter, we shift from the carefully guided world of single-event modeling into the messier, faster, and far more chaotic domain of bulk lightcurve analysis. You'll go from fitting one event at a time to building the infrastructure that lets you handle dozens—eventually thousands—automatically.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> Why Are We Doing This? </font>\n",
        "\n",
        "With the upcoming Roman Space Telescope, the volume of microlensing events is expected to abruptly increase with. Simulations suggest the number of microlensing events observed by Roman, in its lifetime, will be on the order of tens of thousands. Event analysis is expected to become primarily an automated, buck-processing endaevor. Manually tuning each fit just isn't practical. Instead, we need a streamlined approach that lets us quickly process bulk data while still catching those subtle details. That kind of thinking is what we are going to emulate in this chapter.\n",
        "\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> What's in It for You? </font>\n",
        "\n",
        "Our aim is to provide you with a realistic view of what working with bulk microlensing data involves. This chapter is designed to help you build confidence in managing large datasets while also preparing you for the more complex tasks you'll encounter in your future research.\n",
        "\n",
        "You'll simulate the process of fitting a full observing season: downloading data, defining objective functions, writing event finders, applying priors, parallelizing computation, and adapting to subtle data challenges in real time.\n",
        "\n",
        "This isn't just about models—it's about workflow. It's about thinking like a pipeline without losing your scientific instincts.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> What Will You Learn? </font>\n",
        "* How to fit single-lens microlensing events **at scale**\n",
        "* How to structure your code for **repeatability and automation**\n",
        "* How to apply **priors** that penalize unphysical results (like negative flux)\n",
        "* Understand the limitations of simpler fitting methods and see how advanced tools like **emcee** can help you explore complex likelihood spaces.\n",
        "* How to integrate tools like **MulensModel** with your own logic\n",
        "* How to use **pathos** for notebook-friendly parallelization\n",
        "* How to spot when your tools are starting to break—and why that's a feature, not a flaw\n",
        "\n",
        "You'll also get:\n",
        "* Hands-on experience with real OGLE EWS data\n",
        "* The chance to write your own event-finding algorithm\n",
        "* An optional full-season Roman simulation challenge to pull it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CDq3RSSZOwm"
      },
      "source": [
        "<!--\n",
        "### <font face=\"Helvetica\" size=\"5\"> Dev Notes </font>\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzLGrw5buDGB"
      },
      "source": [
        "<!--\n",
        ">  [x] Where are we going to get the data from? WG7?\n",
        ">>> Old data challenge data\n",
        ">\n",
        ">  [x] Where are we hosting this?\n",
        ">>> Static notebooks in the same place as the slides and links to colab.\n",
        ">\n",
        ">  [x] yaml/ install instruction, if we aren't on Colab\n",
        ">>> W are staying in Colab. Install inline.\n",
        ">\n",
        ">  [x] css integration, if we aren't on Colab\n",
        ">>> It works for the static notebook so this could still be worth while.\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxZvpWmxugPV"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> 2. Recap of the Single-Lens Fit </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hpoz8n1CwAEn",
        "outputId": "2f78a805-1c61-4fef-8ef0-6b89a44c3126"
      },
      "outputs": [],
      "source": [
        "#@title Imports and Setup\n",
        "\n",
        "# system tools\n",
        "import os\n",
        "import sys\n",
        "from io import StringIO\n",
        "import time\n",
        "from typing import Tuple, Callable, Optional, List\n",
        "import shutil\n",
        "\n",
        "# data analysis tools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from scipy.optimize import minimize\n",
        "import astropy.units as u\n",
        "from astropy.coordinates import Angle, SkyCoord\n",
        "try:\n",
        "    from google.colab import sheets  # will only work if you are running on Colab\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# web scrapping tools\n",
        "import bs4 as bs\n",
        "import urllib\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "\n",
        "# parallel processing tools\n",
        "!pip install pathos\n",
        "from pathos.multiprocessing import ProcessingPool as Pool  # for multiprocessing inside jupyter\n",
        "import multiprocessing as mp  # Ensure this is imported\n",
        "\n",
        "# microlensing tool\n",
        "!pip install MulensModel\n",
        "import MulensModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FstUqNcu-6uM"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 2.1 Mulens Model Package Fix </font>\n",
        "\n",
        "Follow the instructions in this section **if you haven't already got a working version of Mulens Model**.\n",
        "\n",
        "You can check whether your version is working by generating the plot in [Section 2.2](#section-22)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3akkt4AY1Tox",
        "outputId": "d380318d-112c-4cf1-c146-3191b4a0aaef"
      },
      "outputs": [],
      "source": [
        "#@title Removing the 'data' file inside `mulensmodel_dir`\n",
        "\n",
        "mulensmodel_dir = os.path.dirname(MulensModel.__file__)\n",
        "data_file_path = os.path.join(mulensmodel_dir, 'data')\n",
        "\n",
        "if os.path.exists(data_file_path):\n",
        "  if os.path.isfile(data_file_path):\n",
        "    os.remove(data_file_path)\n",
        "  else:\n",
        "    shutil.rmtree(data_file_path)\n",
        "  print(f\"Removed 'data' file or directory from {mulensmodel_dir}\")\n",
        "else:\n",
        "  print(f\"No 'data' file or directory found in {mulensmodel_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q0hD1I_g-kXB",
        "outputId": "5076de61-47d1-4945-dde4-a9ad860ce0f7"
      },
      "outputs": [],
      "source": [
        "#@title Replace this path with the path printed above, if they are different\n",
        "!ls /usr/local/lib/python3.11/dist-packages/MulensModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MpS9ABwk2VpF",
        "outputId": "5ba2d4fc-5f15-42d0-b68c-39e4d508f651"
      },
      "outputs": [],
      "source": [
        "#@title Clone the MulensModel directory from git\n",
        "!git clone https://github.com/rpoleski/MulensModel.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xPdIHECB7saf",
        "outputId": "3a9c7b14-c8c4-43d2-d270-cd0be6500c8d"
      },
      "outputs": [],
      "source": [
        "#@title Copy the data folder to the package location\n",
        "\n",
        "print('Cuurent Working Directory:')\n",
        "!ls\n",
        "\n",
        "print('\\nMulensModel Directory:')\n",
        "!ls MulensModel\n",
        "\n",
        "# copy the data folder to the package location\n",
        "# you may need to change these file paths, depending on where you run this notebook\n",
        "!cp -r ./MulensModel/data /usr/local/lib/python3.11/dist-packages/MulensModel\n",
        "\n",
        "print('\\nMulensModel Package Directory:')\n",
        "!ls /usr/local/lib/python3.11/dist-packages/MulensModel\n",
        "\n",
        "print('\\ndata Directory:')\n",
        "!ls /usr/local/lib/python3.11/dist-packages/MulensModel/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCbjW0QP4EvL"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress *just that specific warning* because we know what its doing and\n",
        "# we want to keep the output clean\n",
        "warnings.filterwarnings(\"ignore\", message=\".*does not have a limb-darkening coefficient.*\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-r0Prgp_SJq"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 2.2 Single-Lens Fitting with Mulens Model <a id=\"section-22\"></a> </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OII-gvK2yx1U"
      },
      "outputs": [],
      "source": [
        "#@title Available finite source methods\n",
        "\n",
        "finite_source_methods = [\n",
        "    # Uniform source\n",
        "    'finite_source_uniform_Gould94',               # 0, 10E-3 < rho < 1 (has a bug)\n",
        "    'finite_source_uniform_Gould94_direct',        # 1, 10E-3 < rho < 1\n",
        "    'finite_source_uniform_WittMao94',             # 2, rho < 0.01\n",
        "    'finite_source_uniform_Lee09',                 # 3, rho > 0.01\n",
        "\n",
        "    # Limb-darkened source\n",
        "    'finite_source_LD_WittMao94',                  # 4, rho < 0.01\n",
        "    'finite_source_LD_Yoo04',                      # 5, 10E-3 < rho < 1\n",
        "    'finite_source_LD_Yoo04_direct',               # 6, 10E-3 < rho < 1\n",
        "    'finite_source_LD_Lee09'                       # 7, rho > 0.01\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_PL4GWmeJE4"
      },
      "source": [
        "Let's take a look at how different higher-order effects change the magnification model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "K6RPqiiZwGzt",
        "outputId": "a6f53116-e35d-4ccd-d086-e8b5fe68fc33"
      },
      "outputs": [],
      "source": [
        "#@title Plotting the magnification models\n",
        "\n",
        "# plot bounds\n",
        "t_min = 2452750\n",
        "t_max = 2452950\n",
        "t_range = [t_min, t_max]\n",
        "\n",
        "# Model parameters\n",
        "t_0 =  2452848.06\n",
        "u_0 = 0.133\n",
        "t_E = 61.5\n",
        "log_rho = -1.4 #@param {type:\"slider\", min:-3, max:0, step:0.1}\n",
        "rho = 10**log_rho\n",
        "pi_E_E = -0.5 #@param {\"type\":\"slider\",\"min\":-5,\"max\":5,\"step\":0.1}\n",
        "pi_E_N = 1.7 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "t_0_par = 2452848.06 # should not change during modelling and needs to be close to t_0\n",
        "\n",
        "# Define a point source, point lens model\n",
        "pspl = MulensModel.Model({'t_0': t_0, 'u_0': u_0, 't_E': t_E})\n",
        "\n",
        "# Define a finite source, point lens model\n",
        "fspl = MulensModel.Model({'t_0': t_0, 'u_0': u_0, 't_E': t_E, 'rho': rho})\n",
        "\n",
        "# Define a parallax model\n",
        "fspl_pllx = MulensModel.Model({'t_0': t_0,\n",
        "                          'u_0': u_0,\n",
        "                          't_E': t_E,\n",
        "                          'rho': rho,\n",
        "                          'pi_E_E': pi_E_E,\n",
        "                          'pi_E_N': pi_E_N,\n",
        "                          't_0_par': t_0_par  # fixed value for parallax calculations: ~t_0\n",
        "                          },\n",
        "                         ra='18:04:45.71',\n",
        "                         dec='-26:59:15.2'\n",
        "                         )\n",
        "\n",
        "# Plot the magnification curve:\n",
        "plt.close(0)\n",
        "plt.figure(0)\n",
        "pspl.plot_magnification(\n",
        "    t_range=t_range,\n",
        "    subtract_2450000=True,\n",
        "    color='grey',\n",
        "    linestyle='-',\n",
        "    label='PSPL'\n",
        "    )\n",
        "\n",
        "# calculate the magnification curve using a finite source model\n",
        "fspl.set_magnification_methods([2450000., finite_source_methods[1], 2470000.])  # rho = 0.1\n",
        "fspl.plot_magnification(\n",
        "    t_range=t_range,\n",
        "    subtract_2450000=True,\n",
        "    color='blue',\n",
        "    linestyle='--',\n",
        "    label='FSPL with uniform-brightness'\n",
        "    )\n",
        "\n",
        "# calculate the magnification curve using a finite source model with limb darkening\n",
        "fspl.set_magnification_methods([2450000., finite_source_methods[5], 2470000.])  # rho = 0.1\n",
        "fspl.plot_magnification(\n",
        "    t_range=t_range,\n",
        "    subtract_2450000=True,\n",
        "    color='red',\n",
        "    linestyle=':',\n",
        "    linewidth=2,\n",
        "    label='FSPL with Limb Darkening'\n",
        "    )\n",
        "\n",
        "# calculate the magnification curve using a finite source model and parallax\n",
        "fspl_pllx.set_magnification_methods([2450000., finite_source_methods[1], 2470000.])  # rho = 0.1\n",
        "fspl_pllx.plot_magnification(\n",
        "    t_range=t_range,\n",
        "    subtract_2450000=True,\n",
        "    color='green',\n",
        "    linestyle=':',\n",
        "    linewidth=2,\n",
        "    label='FSPL with parallax'\n",
        "    )\n",
        "\n",
        "\n",
        "# calculate the u_0 finite-source, parallax solution\n",
        "fspl_pllx.set_magnification_methods([2450000., finite_source_methods[3], 2470000.])\n",
        "parameters = [\"t_0\", \"u_0\", \"t_E\", \"rho\", \"pi_E_E\", \"pi_E_N\", \"t_0_par\"]\n",
        "setattr(fspl_pllx.parameters, \"u_0\", -u_0)  # multiply u0 by -1\n",
        "fspl_pllx.plot_magnification(\n",
        "    t_range=t_range,\n",
        "    subtract_2450000=True,\n",
        "    color='purple',\n",
        "    linestyle='--',\n",
        "    linewidth=2,\n",
        "    label=r'-$u_0$ FSPL with parallax'\n",
        "    )\n",
        "\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.04, 1), borderaxespad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnDGc3ejACDP"
      },
      "source": [
        "If your version of Mulens model is working, this figure should have rendered without an error.\n",
        "\n",
        "> There are a few things to take away from this plot:\n",
        "> * the finite source effect has a big effect on the shape of the magnification curve\n",
        "> * the surface brightness model (e.g., uniform) for the source has much less of an effect\n",
        "> * the degenerate parallax solutions may be noticably different with sufficiently large parallax\n",
        "> * parallax does not need to be as big, for the affect to noticably change the magnification curve, compared with a static model.\n",
        "\n",
        "> **Exercise 1**\n",
        ">\n",
        "> Try playing with the parallax (`\"pi_E_N\"`, `\"pi_E_E\"`) and finite source (`\"rho\"`)parameters and see how they  change your magnification model.\n",
        ">\n",
        "> *Note. This is not an interactive plot. You have to run the cell again after moving the slider.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSTBwBCFKqhQ"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> 3. OGLE EWS Bulk Lightcurve Fit </font>\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "The Roman style data have many epoch which make the evaluation of a magnification model take a long time. Ground -based data, on the other hand have much fewer epochs and alert pages, such as OGLEs EWS, which estimate fit parameters for us. We are going to design our bulk fit on these much more tractible data and then apply it to a simulated Roman season later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48FwbafF0ddy"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.1 Getting the ground-based data <a id=\"section-22\"></a> </font>\n",
        "\n",
        "Let's start this process by scraping for some lightcurves and microlensing model parameter estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LnKQvQEF4km",
        "outputId": "c397091f-64dc-4f0c-f2ff-4fb4de2935bb"
      },
      "outputs": [],
      "source": [
        "#@title Web scrapping functions\n",
        "\n",
        "def get_data_url(event: str) -> str:\n",
        "    '''Takes an event name and returns the URL for the data page.'''\n",
        "\n",
        "    event = event.split('-') # split the event name into its components, seperated by '-'\n",
        "    year = event[0]  # the first component is the year\n",
        "    region = event[1].lower()  # the second component is region (e.g., blg or gd), which we need to make lower case.\n",
        "    number = event[2]  #\n",
        "    url = f'https://www.astrouw.edu.pl/ogle/ogle4/ews/{year}/{region}-{number}/phot.dat'\n",
        "\n",
        "    return url\n",
        "\n",
        "def fetch_event_data(url: str) -> pd.DataFrame:\n",
        "    '''Takes a url and returns the data as a pandas dataframe.'''\n",
        "\n",
        "    # Read the data from the URL\n",
        "    response = urllib.request.urlopen(url)\n",
        "    data = response.read().decode('utf-8')\n",
        "\n",
        "    # Convert the data to a pandas DataFrame\n",
        "    #df = pd.read_csv(StringIO(data), delim_whitespace=True, header=None, names=['HJD', 'I magnitude', 'magnitude error', 'seeing', 'sky level'])\n",
        "    df = pd.read_csv(StringIO(data), sep=r'\\s+', header=None, names=['HJD', 'I magnitude', 'magnitude error', 'seeing', 'sky level'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Test\n",
        "event = '2017-BLG-0001'\n",
        "event_data_url = get_data_url(event)\n",
        "data = fetch_event_data(event_data_url)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUMaAZzyJYok"
      },
      "source": [
        "Great. Now that we have functions to fetch lightcurves from the OGLE EWS website, we can just grab those when we need them. But we also need some good parameter estimates if we don't want our fits to take eternity to run, or potentially fail to find the likelihood maximum. Estimating these parameters is part of the EWS process, so we can shamlessly steal from those for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n_oCJwoCGHuq",
        "outputId": "c92b1bb0-4002-49ae-c4f7-78148e069bb2"
      },
      "outputs": [],
      "source": [
        "#@title More web scraping (this time for the EWS table)\n",
        "\n",
        "def fetch_table_data(url):\n",
        "    '''Takes a URL and returns the first table as a pandas DataFrame.'''\n",
        "    source = urllib.request.urlopen(url).read()\n",
        "    soup = bs.BeautifulSoup(source, 'lxml')\n",
        "    table = soup.find_all('table')\n",
        "    df = pd.read_html(StringIO(str(table)))[0]\n",
        "\n",
        "    return df\n",
        "\n",
        "ews_url = \"https://ogle.astrouw.edu.pl/ogle4/ews/ews.html\"  # https://ogle.astrouw.edu.pl/ogle4/ews/2024/ews.html for last year\n",
        "ews_df = fetch_table_data(ews_url)\n",
        "print(ews_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTNBMc8oK5Kh"
      },
      "source": [
        "Let's just loop through all the events and add the data URLs to the pandas dataframe, for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uw0PfdBvGLU4",
        "outputId": "e5955df0-ec27-4bf9-e9c3-af02b53dd997"
      },
      "outputs": [],
      "source": [
        "#@title Adding a URL column to the data frame\n",
        "\n",
        "# Add a new column to the EWS data frame ('ews_df'), using the column name 'event data url'.\n",
        "ews_df['event data url'] = ews_df['Event'].apply(get_data_url)\n",
        "print(ews_df)\n",
        "print(min(ews_df['Tmax (HJD)']), max(ews_df['Tmax (HJD)']))\n",
        "print(min(ews_df['Umin']), max(ews_df['Umin']))\n",
        "print(min(ews_df['tau']), max(ews_df['tau']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfpfmFgoLF_p"
      },
      "source": [
        "Nice! Now we have some data to test on. And some web scraping skills under our belts too. We should quickly test this with a single lightcurve, and plot the model from the OGLE EWS table.\n",
        "\n",
        "> **Exercise 2**\n",
        ">\n",
        "> Add parallax to the model and plot below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "qcEnAWfQLbSe",
        "outputId": "daf6df51-68ef-4a13-fae2-2cdfeaed74f7"
      },
      "outputs": [],
      "source": [
        "#@title Plotting the '2025-BLG-0001' event with the EWS model\n",
        "\n",
        "###### [ ] add parallax to the model.\n",
        "\n",
        "# Function to process a single event\n",
        "def plot_event_data(i, ews_df):\n",
        "    event = ews_df['Event'][i]\n",
        "    print(event)\n",
        "    print(ews_df.columns)\n",
        "    url = ews_df['event data url'][i]\n",
        "    data = fetch_event_data(url)\n",
        "    t_0_0 = ews_df['Tmax (HJD)'][i]\n",
        "    u_0_0 = ews_df['Umin'][i] * 1.1  # initial guess\n",
        "    t_E_0 = ews_df['tau'][i] * 1.1  # initial guess\n",
        "    rho_0 = 0.001  # initial guess\n",
        "    ######\n",
        "    # pi_E_E =\n",
        "    # pi_E_N =\n",
        "    # t_0_par =\n",
        "    ######\n",
        "\n",
        "\n",
        "    plt.close(i+1)\n",
        "    plt.figure(i+1)\n",
        "\n",
        "    plt.errorbar(data['HJD'],\n",
        "                  data['I magnitude'],\n",
        "                  yerr=data['magnitude error'],\n",
        "                  fmt='x',\n",
        "                  color='black'\n",
        "                  )\n",
        "    plt.axvline(ews_df['Tmax (HJD)'][i], color='blue', linestyle='--', label='EWS Tmax')\n",
        "\n",
        "    plt.title(event)\n",
        "    plt.xlabel('HJD')\n",
        "    plt.ylabel('I magnitude')\n",
        "\n",
        "    # Data as a list of numpy arrays\n",
        "    data_list = [data['HJD'].to_numpy(), data['I magnitude'].to_numpy(), data['magnitude error'].to_numpy()]\n",
        "\n",
        "    # Pack everything into MulensModel objects\n",
        "    data_object = MulensModel.MulensData(data_list=data_list,\n",
        "                                plot_properties={'color': 'thistle',\n",
        "                                                 'label': 'OGLE',\n",
        "                                                 'marker': 'x',\n",
        "                                                 'markersize': 2\n",
        "                                                 },\n",
        "                                phot_fmt='mag',\n",
        "                                bandpass='I'\n",
        "                                )\n",
        "    ######\n",
        "    fspl_model = MulensModel.Model({'t_0': t_0_0,\n",
        "                                    'u_0': u_0_0,\n",
        "                                    't_E': t_E_0,\n",
        "                                    'rho': rho_0}\n",
        "                                   )  # add parallax parameters\n",
        "                                      # (initialize with (0.0, 0.0))\n",
        "    ######\n",
        "    fspl_model.set_magnification_methods([t_0_0 - 3.0 * t_E_0,\n",
        "                                          finite_source_methods[0],\n",
        "                                          t_0_0 + 3.0 * t_E_0\n",
        "                                          ],\n",
        "                                          source=None\n",
        "                                          )  # rho <= 0.1\n",
        "    event_object = MulensModel.Event(datasets=data_object, model=fspl_model)\n",
        "\n",
        "    ######\n",
        "    parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\", \"rho\"]  # add parallax parameters\n",
        "\n",
        "    ######\n",
        "\n",
        "    # Plot the initial model\n",
        "    ######\n",
        "    label = 'EWS model: %1.3f, %1.3f, %1.3f, %1.3f' %(t_0_0,\n",
        "                                                      u_0_0,\n",
        "                                                      t_E_0,\n",
        "                                                      rho_0\n",
        "                                                      )  # Add parallax to the\n",
        "                                                         # plot label\n",
        "    ######\n",
        "    event_object.plot_model(color='r',\n",
        "                            linestyle=':',\n",
        "                            t_range=[min(data['HJD']),\n",
        "                                     max(data['HJD'])\n",
        "                                    ],\n",
        "                            label=label\n",
        "                            )\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig(f'./{event}.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_event_data(0, ews_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXXGwp4YLqGA"
      },
      "source": [
        "Is it working?\n",
        "\n",
        "If so, we can move on and test our fitting algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRmBCne305Qu"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.2 Fitting basic PSPL model to <a id=\"section-22\"></a> </font>\n",
        "\n",
        "First, we’ll need an **objective function** — a way to measure how well our model fits the data (or, more precisely, how likely the model is to have generated the data, assuming Gaussian noise)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CZpDATdEIVl"
      },
      "outputs": [],
      "source": [
        "#@title Objective function\n",
        "def mulens_neglogP_function(theta, parameters_to_fit, event, verbose=False):\n",
        "    ''' negative log prob function for MulensModel fitting '''\n",
        "\n",
        "    # Create a dictionary from theta values for easier access\n",
        "    params = dict(zip(parameters_to_fit, theta))\n",
        "\n",
        "    # unpack params\n",
        "    t_0_value = params['t_0']\n",
        "    u_0_value = params['u_0']\n",
        "    t_E_value = params['t_E']\n",
        "    # rho is handled later\n",
        "\n",
        "    # Prior Checks\n",
        "    if not ((2460600) <= t_0_value <= (2461000)):  # this needs to change if you\n",
        "                                                   # use this code to fit a different\n",
        "                                                   # season\n",
        "                                                   # For example, for exercies X\n",
        "                                                   # !!!\n",
        "        return np.inf\n",
        "    if not (0.000001 <= u_0_value <= 2.5):  # u_0 can't be 0\n",
        "        return np.inf\n",
        "    if not (0.1 <= t_E_value <= 700):\n",
        "        return np.inf\n",
        "\n",
        "    # Handle rho or log_rho prior\n",
        "    if 'rho' in params:\n",
        "        rho_value = params['rho']\n",
        "        if not (0 <= rho_value <= 0.2): # Assuming rho >= 0 is desired\n",
        "            return np.inf\n",
        "    elif 'log_rho' in params:\n",
        "        log_rho_value = params['log_rho']\n",
        "        # Check log_rho lower bound first\n",
        "        if log_rho_value < -10: # Check the log value directly\n",
        "             return np.inf\n",
        "        rho_value = 10**log_rho_value # Convert to rho for the upper bound check\n",
        "        if rho_value > 0.2: # Check rho upper bound\n",
        "             return np.inf\n",
        "\n",
        "    # Update Model Parameters\n",
        "    # If all prior checks passed, NOW update the model\n",
        "    setattr(event.model.parameters, 't_0', t_0_value)\n",
        "    setattr(event.model.parameters, 'u_0', u_0_value)\n",
        "    setattr(event.model.parameters, 't_E', t_E_value)\n",
        "    setattr(event.model.parameters, 'rho', rho_value)\n",
        "    # Add other parameters if needed\n",
        "\n",
        "    # Example flux priors\n",
        "    penalty = 0.0\n",
        "    '''\n",
        "    dataset = event.datasets[0]\n",
        "    event.fit_fluxes() # This needs the model parameters to be set correctly\n",
        "    ([FS], FB) = event.get_flux_for_dataset(dataset)\n",
        "\n",
        "    if verbose:\n",
        "        print(f'FS: {FS}, FB: {FB}')\n",
        "\n",
        "    # Flux priors (check AFTER fitting fluxes)\n",
        "    if FB <= 0:\n",
        "        penalty = ((FB / 100)**2) # why 100? I told you, vibes.\n",
        "    if FS <= 0 or (FS + FB) <= 0:\n",
        "        return np.inf # Return inf if fluxes are non-physical '''\n",
        "\n",
        "    # Calculate Chi2\n",
        "    chi2 = event.get_chi2()\n",
        "    if verbose:\n",
        "        print('chi2 = ', chi2)\n",
        "\n",
        "    # Return the objective function value (negative log likelihood ~ chi2/2)\n",
        "    # Scipy minimize finds the minimum, so we return chi2 (or chi2/2)\n",
        "    return chi2/2.0 + penalty # Technically negLogP is chi2/2 + constant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XruXVcLsAuAe"
      },
      "source": [
        "> **Exercise 3**\n",
        ">\n",
        "> Test out a fit of a subsample of the EWS data, by running the following 2 cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ouq9PSahGXRZ"
      },
      "outputs": [],
      "source": [
        "#@title Function to process a single event\n",
        "def process_event(i, ews_df, n, start_time, verbose=True, log_rho_prior=True):\n",
        "\n",
        "    # Event stuff\n",
        "    event = ews_df['Event'][i]\n",
        "    event_start_time = time.time()\n",
        "    url = ews_df['event data url'][i]\n",
        "    data = fetch_event_data(url)\n",
        "\n",
        "    # Model stuff\n",
        "    t_0_0 = ews_df['Tmax (HJD)'][i]\n",
        "    # We are going to change the next guesses from those provided by the EWS, to\n",
        "    # give our optiizer some room to move\n",
        "    u_0_0 = ews_df['Umin'][i] * 1.1  # initial guess\n",
        "    t_E_0 = ews_df['tau'][i] * 1.1  # initial guess\n",
        "    rho_0 = 0.001  # initial guess\n",
        "\n",
        "    # Crop the data so that we don't have to fit 10 years worth\n",
        "    t_window = (t_0_0 - 10.0 * t_E_0, t_0_0 + 10.0 * t_E_0)\n",
        "    t_window = (max(min(data['HJD']), t_window[0]), min(max(data['HJD']), t_window[1]))\n",
        "    # This creates a boolean mask: True for rows within the window, False otherwise\n",
        "    time_mask = (data['HJD'] >= t_window[0]) & (data['HJD'] <= t_window[1])\n",
        "    # Apply the mask to filter the DataFrame\n",
        "    data = data[time_mask]\n",
        "\n",
        "    if t_E_0 > 50:  # liekly remnant (they do derpy stuff in the wings)\n",
        "        mag_method = finite_source_methods[0]  # Note: `finite_source_uniform_Gould94`\n",
        "                                               # is not ideal for events where\n",
        "                                               # rho ≲ 1e-3, but we use it here\n",
        "                                               # to ensure fast and consistent\n",
        "                                               # modeling while testing functions.\n",
        "                                               # For high-mass lenses or precision\n",
        "                                               # modeling, consider switching to\n",
        "                                               # `WittMao94` variants (method 2\n",
        "                                               # or 4).\n",
        "    else:  # likely MS star\n",
        "        mag_method = finite_source_methods[0]\n",
        "\n",
        "    if verbose:\n",
        "        print('\\n', event)\n",
        "        print('-----------------')\n",
        "        print(f't_0_0: {t_0_0:1.3f}')\n",
        "        print(f'u_0_0: {u_0_0:1.3f}')\n",
        "        print(f't_E_0: {t_E_0:1.3f}')\n",
        "        print(f'rho_0: {rho_0:1.5f}')\n",
        "        print(f'Method = {mag_method}')\n",
        "        print('-----------------')\n",
        "        print(f'Time started: {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}')\n",
        "\n",
        "    if i % n == 0:\n",
        "        plt.close(i)\n",
        "        plt.figure(i)\n",
        "\n",
        "        plt.errorbar(data['HJD'],\n",
        "                     data['I magnitude'],\n",
        "                     yerr=data['magnitude error'],\n",
        "                     fmt='x',\n",
        "                     color='black'\n",
        "                     )\n",
        "        plt.axvline(ews_df['Tmax (HJD)'][i], color='blue', linestyle='--', label=r'EWS $T_{max}$')\n",
        "\n",
        "        plt.title(event)\n",
        "\n",
        "    # Data as a list of numpy arrays\n",
        "    data_list = [data['HJD'].to_numpy(), data['I magnitude'].to_numpy(), data['magnitude error'].to_numpy()]\n",
        "\n",
        "    # Pack everything into MulensModel objects\n",
        "    data_object = MulensModel.MulensData(data_list=data_list,\n",
        "                                plot_properties={'color': 'thistle',\n",
        "                                                 'label': 'OGLE',\n",
        "                                                 'marker': 'x',\n",
        "                                                 'markersize': 2\n",
        "                                                 },\n",
        "                                phot_fmt='mag',\n",
        "                                bandpass='I'\n",
        "                                )\n",
        "    fspl_model = MulensModel.Model({'t_0': t_0_0, 'u_0': u_0_0, 't_E': t_E_0, 'rho': rho_0})\n",
        "\n",
        "    fspl_model.set_magnification_methods([t_window[0],\n",
        "                                          mag_method,\n",
        "                                          t_window[1]\n",
        "                                          ],\n",
        "                                          source=None\n",
        "                                          )  # rho <= 0.1\n",
        "    event_object = MulensModel.Event(datasets=data_object, model=fspl_model)\n",
        "\n",
        "    # Plot the initial model\n",
        "    if i % n == 0:\n",
        "        event_object.plot_model(color='r',\n",
        "                                linestyle=':',\n",
        "                                t_range=[min(data['HJD']),\n",
        "                                         max(data['HJD'])\n",
        "                                         ],\n",
        "                                label='Initial Guess: %1.3f, %1.3f, %1.3f, %1.5f' %(t_0_0, u_0_0, t_E_0, rho_0)\n",
        "                                )\n",
        "\n",
        "    # get initial chi2\n",
        "    initial_chi2 = event_object.get_chi2()\n",
        "\n",
        "    # index order\n",
        "    parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\", \"rho\"]\n",
        "    if log_rho_prior:\n",
        "        parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\", \"log_rho\"]\n",
        "\n",
        "    # Fit using scipy Nelder-Mead\n",
        "    if log_rho_prior:\n",
        "        result = minimize(mulens_neglogP_function,\n",
        "                          [t_0_0, u_0_0, t_E_0, np.log10(rho_0)],\n",
        "                          args=(parameters_to_fit,\n",
        "                          event_object),\n",
        "                          method='Nelder-Mead'\n",
        "                          ) # log rho prior\n",
        "    else:\n",
        "        result = minimize(mulens_neglogP_function,\n",
        "                          [t_0_0, u_0_0, t_E_0, rho_0],\n",
        "                          args=(parameters_to_fit,\n",
        "                          event_object),\n",
        "                          method='Nelder-Mead'\n",
        "                          )\n",
        "\n",
        "    # Make sure the values in event_object are the best fit values\n",
        "    neglogP_final = mulens_neglogP_function(result.x, parameters_to_fit, event_object, verbose=False)\n",
        "\n",
        "    if verbose:\n",
        "        print('-----------------')\n",
        "        print(f'Time elapsed: {time.time() - event_start_time} seconds')\n",
        "        print('-----------------')\n",
        "        neglogP_final = mulens_neglogP_function(result.x, parameters_to_fit, event_object, verbose=True)\n",
        "        print('-----------------')\n",
        "        print(f't_0: {event_object.model.parameters.t_0:1.3f} ({result.x[0]:1.3f})')\n",
        "        print(f'u_0: {event_object.model.parameters.u_0:1.3f} ({result.x[1]:1.3f})')\n",
        "        print(f't_E: {event_object.model.parameters.t_E:1.3f} ({result.x[2]:1.3f})')\n",
        "        print(f'rho: {event_object.model.parameters.rho:1.5f} ({result.x[3]:1.5f})') # Always rho here\n",
        "        print('-----------------')\n",
        "        print(f'Final -logP check: {neglogP_final}')\n",
        "        print('-----------------')\n",
        "        print(f'Initial chi2 (from initial guess): {initial_chi2}')\n",
        "        print(f'Final chi2 (from event object): {event_object.get_chi2()}')\n",
        "        print(f'Delta chi2: {event_object.get_chi2() - initial_chi2}')\n",
        "        print('Delta chi2/dof: ',(event_object.get_chi2() - initial_chi2) / (len(data['HJD']) - 4))\n",
        "        print('-----------------')\n",
        "\n",
        "\n",
        "    # Plot the fit model and show (event_object now has correct params)\n",
        "    if i % n == 0:\n",
        "        # Construct the label string from the corrected event_object model state\n",
        "        label = '''Best Fit: {0.t_0:1.3f}, {0.u_0:1.3f}, {0.t_E:1.3f},\n",
        "                   {0.rho:1.5f}'''.format(event_object.model.parameters)\n",
        "\n",
        "        event_object.plot_model(color='r',\n",
        "                                linestyle='-',\n",
        "                                t_range=[min(data['HJD']), max(data['HJD'])],\n",
        "                                label=label\n",
        "                                )\n",
        "        # Use parameters from the event object model for plot limits etc.\n",
        "        t_0 = event_object.model.parameters.t_0\n",
        "        t_E = event_object.model.parameters.t_E\n",
        "        # Define plot window based on final params or keep original? Your choice.\n",
        "        # Using original t_window:\n",
        "        plt.xlim(max(min(data['HJD']), t_window[0]),\n",
        "                 min(max(data['HJD']), t_window[1])\n",
        "                 )\n",
        "        plt.legend(loc='upper left')\n",
        "        plt.xlabel('HJD')\n",
        "        plt.ylabel('I magnitude')\n",
        "        plt.savefig(f'./{event}.png', bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    # Return the actual best-fit parameters found by minimize\n",
        "    # You might want to return the converted values for consistency\n",
        "    final_params_values = list(result.x)\n",
        "    if log_rho_prior:\n",
        "        log_rho_idx = parameters_to_fit.index('log_rho')\n",
        "        final_params_values[log_rho_idx] = 10**final_params_values[log_rho_idx] # Convert log_rho back to rho\n",
        "\n",
        "    event_object.fit_fluxes() # This needs the model parameters to be set correctly\n",
        "    ([FS], FB) = event_object.get_flux_for_dataset(event_object.datasets[0])\n",
        "    if verbose:\n",
        "        print(f'FS: {FS}, FB: {FB}')\n",
        "        print('-----------------')\n",
        "    final_params_values = [FS, FB] + final_params_values\n",
        "\n",
        "    return i, final_params_values # Return values, maybe with rho always as rho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "NrGby0EuAQYa",
        "outputId": "4a5006ca-ca20-4690-8efc-6e3b72e4a5ae"
      },
      "outputs": [],
      "source": [
        "#@title Test fitting\n",
        "\n",
        "# numpy array for the fit params\n",
        "fit_params = np.zeros((ews_df.shape[0], 8))\n",
        "\n",
        "plot_fraction = 0.1  # fraction of events to show plots for\n",
        "N = 10  # number of events to fit\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(N):\n",
        "    i, result = process_event(i,\n",
        "                              ews_df,\n",
        "                              int(1/plot_fraction),\n",
        "                              start_time,\n",
        "                              verbose=True,\n",
        "                              log_rho_prior=True\n",
        "                              )\n",
        "    n = len(result)\n",
        "    fit_params[i, :n] = result\n",
        "\n",
        "# Geez, how long is this going to take?\n",
        "time_at_N = time.time()\n",
        "time_for_N = time_at_N - start_time\n",
        "print(f'Time taken to fit the first {N}: {time_for_N} seconds')\n",
        "\n",
        "# Estimate completion time\n",
        "completion_time = time_at_N + (ews_df.shape[0] - N) / N * time_for_N\n",
        "print(time_at_N, (ews_df.shape[0] - N) / N, time_for_N)\n",
        "\n",
        "# Print the completion time in human readable format hr:min:sec\n",
        "print(f'Estimated season completion time: {time.strftime(\"%H:%M:%S\", time.localtime(completion_time))}')\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1XCvoKGnK2"
      },
      "source": [
        "Let's just take a look at the parameter distributions and see what that did and if the results look reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7R0OStnUGlJJ",
        "outputId": "97692dc9-7c3e-429d-aecc-3c10cc6f7dac"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 3 fit parameter distributions\n",
        "\n",
        "def plot_histograms(fit_params, exercise):\n",
        "    ''' '''\n",
        "\n",
        "    plt.close(100 + exercise)\n",
        "    plt.figure(100 + exercise)\n",
        "    fig, axes = plt.subplots(8, 1, figsize=(5, 25))\n",
        "\n",
        "    # FS values\n",
        "    min_FS = min(fit_params[:, 0])\n",
        "    max_FS = max(fit_params[:, 0])\n",
        "    print(min_FS, max_FS)\n",
        "    axes[0].hist(fit_params[:, 0], bins=20, color='#361d49')\n",
        "    axes[0].set_xlabel(r'$F_\\text{S}$')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    #axes[0].set_yscale('log')\n",
        "\n",
        "    # FB values\n",
        "    min_FB = min(fit_params[:, 1])\n",
        "    max_FB = max(fit_params[:, 1])\n",
        "    print(min_FB, max_FB)\n",
        "    axes[1].hist(fit_params[:, 1], bins=20, color='#361d49')\n",
        "    axes[1].set_xlabel(r'$F_\\text{B}$')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    #axes[1].set_yscale('log')\n",
        "\n",
        "    # t_0 values\n",
        "    min_t0 = min(fit_params[:, 2])\n",
        "    max_t0 = max(fit_params[:, 2])\n",
        "    print(min_t0, max_t0)\n",
        "    axes[2].hist(fit_params[:, 2], bins=20, color='#361d49')\n",
        "    axes[2].set_xlabel(r'$t_0$')\n",
        "    axes[2].set_ylabel('Frequency')\n",
        "    #log the y axis\n",
        "    #axes[2].set_yscale('log')\n",
        "\n",
        "    # u0 values\n",
        "    min_u0 = min(fit_params[:, 3])\n",
        "    max_u0 = max(fit_params[:, 3])\n",
        "    print(min_u0, max_u0)\n",
        "    axes[3].hist(fit_params[:, 3], bins=20, color='#361d49')\n",
        "    axes[3].set_xlabel(r'$u_0$')\n",
        "    axes[3].set_ylabel('Frequency')\n",
        "    #axes[3].set_yscale('log')\n",
        "\n",
        "    # tE values\n",
        "    min_tE = min(fit_params[:, 4])\n",
        "    max_tE = max(fit_params[:, 4])\n",
        "    print(min_tE, max_tE)\n",
        "    axes[4].hist(fit_params[:, 4], bins=20, color='#361d49')\n",
        "    axes[4].set_xlabel(r'$t_\\text{E}$')\n",
        "    axes[4].set_ylabel('Frequency')\n",
        "    #axes[4].set_yscale('log')\n",
        "\n",
        "    # rho values\n",
        "    min_rho = min(fit_params[:, 5])\n",
        "    max_rho = max(fit_params[:, 5])\n",
        "    print(min_rho, max_rho)\n",
        "    print(np.log10(min_rho), np.log10(max_rho))\n",
        "    axes[5].hist(np.log10(fit_params[:, 5]), bins=20, color='#361d49')\n",
        "    axes[5].set_xlabel(r'$\\rho$')\n",
        "    axes[5].set_ylabel('Frequency')\n",
        "    #axes[5].set_yscale('log')\n",
        "\n",
        "    # pi_E_E values\n",
        "    min_FS = min(fit_params[:, 6])\n",
        "    max_FS = max(fit_params[:, 6])\n",
        "    print(min_FS, max_FS)\n",
        "    axes[6].hist(fit_params[:, 6], bins=20, color='#361d49')\n",
        "    axes[6].set_xlabel(r'$\\pi_{\\text{E},E}$')\n",
        "    axes[6].set_ylabel('Frequency')\n",
        "    #axes[6].set_yscale('log')\n",
        "\n",
        "    # pi_E_N values\n",
        "    min_FB = min(fit_params[:, 7])\n",
        "    max_FB = max(fit_params[:, 7])\n",
        "    print(min_FB, max_FB)\n",
        "    axes[7].hist(fit_params[:, 7], bins=20, color='#361d49')\n",
        "    axes[7].set_xlabel(r'$\\pi_{\\text{E},N}$')\n",
        "    axes[7].set_ylabel('Frequency')\n",
        "    #axes[7].set_yscale('log')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "fit1_params = fit_params[:N].copy()\n",
        "\n",
        "plot_histograms(fit1_params, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9gl76c4S9c7"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.3 Priors </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCQiM8RpTJpT"
      },
      "source": [
        "We will now interpret the deeply non-physical result where the background flux is very negative.\n",
        "\n",
        "Likely at least a few of your events had negative $F_{\\textrm{B}}$ (*we're looking at you, 2025-BLG-0030*). This is a pretty common issue with single-lens modelling.\n",
        "\n",
        "But what does it mean?\n",
        "\n",
        "Nothing that makes any physical sense. If the blend is a little bit negative, that can be explained by systematics in the photometry (e.g. the background was measured with very faint star in it, so our flux scale was zeroed slightly wrong), but there is no reason for the blend to be very negative; we can't detect anti-photons.\n",
        "\n",
        "So, how can we punish the optimizer for its sins? It's actually pretty simple. We punish it by adding a penalty to the objective function. If the penalty is too abrupt, it can sometimes interfere with a gradient descent optimizer's ability to calculate gradients. But if we put a gradual penalty on it, the penalty acts to lead the optimizer in the right direction. The exact shape of the penalty doesn't really matter, it just needs to make more sense than giant negative fluxes.\n",
        "\n",
        "This kind of penalty behaviour is called a prior. Where we use prior knowledge to inform the most probability landscape. Basically we tell optimization, “a very negative blend is dumb - don't do that.”\n",
        "\n",
        "You might argue that you would prefer to approach your modelling from an agnostic perspective and I appreciate your integrity. The problem with that argument though is the assumption that no prior is agnostic. Because no prior is like telling your optimizer, \"negative blend is just as reasonable as positive blend.\" That's not agnostic. You've still informed the fit. You just informed it that every solution was equally possible, which you know it's not. This not-actually-agnostic prior, where you don't code in any sort of penalties, is called a uniform prior. The concept of \"no prior\" is a fallacy.\n",
        "\n",
        "So let's put a reasonable prior on the blend flux so that it stops acting up. Our other fit parameters can continue to have truncated priors (bounds).\n",
        "\n",
        "The next question you might ask is, \"how do I know which prior is the right prior\" the answer to that is: vibes.\n",
        "I'm not even joking. You choose a prior that is physically informed or informed by your \"prior knowledge\" of what your solution should be and don't worry overly much about it. You should however, always be wary that your priors are not so strong that they dominate the fit. If what you get out from a fit is very similar to your prior, that is an indication that you have been too heavy handed and you need to loosen the leash.\n",
        "\n",
        "For example, if you made your prior a hard bound, requiring $F_{\\textrm{B}}>0$, and your best fit solution was $F_{\\textrm{B}}\\approx0$, you have likely stopped the optimizer from exploring valid parameter space.\n",
        "\n",
        "> **Exercise 4**\n",
        ">\n",
        "> Edit `mulens_neglogP_function` to use a gaussian prior to constrain the blend flux, if it is below 0; use a piecewise prior combining:\n",
        ">\n",
        "> * A uniform prior for $F_\\textrm{B}>0$\n",
        "> * A Gaussian penalty for $F_\\textrm{B}<0$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tSAMNBaKGdvB",
        "outputId": "615c1a2d-ce08-4cea-a4fe-68ff5b739a47"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 4 fit parameter distributions\n",
        "\n",
        "fit2_params = fit_params[:N].copy()\n",
        "plot_histograms(fit2_params, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALusqxtSHSpb"
      },
      "source": [
        "> **Exercise 5**\n",
        ">\n",
        "> If everything is working, try adding parallax to model in the EWS event processing function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UPD364REIby6",
        "outputId": "423424a0-a5b5-4dcc-bde7-878fc0b39bec"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 5 fit parameter distributions\n",
        "\n",
        "fit3_params = fit_params[:N].copy()\n",
        "plot_histograms(fit3_params, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIZpeMbHVpj"
      },
      "source": [
        "> **Exercise 6**\n",
        ">\n",
        "> Increase the number of events you are testing on (`N`) so that we can do some more robust benchmarking. You should aim for a number of events that means the above cells takes about 5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2eVVCXcjI1mo",
        "outputId": "eb3ce953-44ba-407b-c1e9-d72f3bc107e3"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 6 fit parameter distributions\n",
        "\n",
        "fit4_params = fit_params[:N].copy()\n",
        "plot_histograms(fit4_params, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8FUNkSQSmJj"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.4 Parallel Processing </font>\n",
        "\n",
        "We are going to demonstrate speeding this process up with parallel processing. However, there are a few things you should know first.\n",
        "\n",
        "We're going to demonstrate how to speed up batch fitting using parallel processing. But before we dive in, there are a few things you should know.\n",
        "\n",
        "If you're running this notebook on Google Colab, your code is executing on a cloud-based virtual machine. Colab typically allocates only two CPU cores per session. While we can parallelize across these, the speed-up won't be dramatic.\n",
        "\n",
        "If you have Jupyter and Python installed on your local machine, you can run this notebook there to take advantage of your full hardware.\n",
        "\n",
        "Want to keep the Colab interface but use your local computer's power?\n",
        "You can connect to a local runtime by following [these instructions](https://research.google.com/colaboratory/local-runtimes.html).\n",
        "This allows Colab to use your local machine's resources while maintaining the notebook interface.\n",
        "\n",
        "⚠️ If you choose this route, your notebook will use local packages. You should install the required environment using the provided `.yml` file and `conda` (or your preferred environment manager). See the setup instructions at the start of this notebook.\n",
        "\n",
        "This notebook uses the `pathos` package for parallelization.\n",
        "In scripts, you'll more commonly see `multiprocessing` used—it has a very similar interface. However, multiprocessing has known issues inside Jupyter notebooks, which is why we're using pathos instead.\n",
        "\n",
        "Pathos is great for educational work in notebooks, but it has quirks. For example:\n",
        "* It caches the function you give it the first time you run it.\n",
        "* If you change that function afterward, it won't update unless you restart the kernel.\n",
        "\n",
        "That's why we finalized our `fit_event()` function earlier—so we could parallelize it now with confidence.\n",
        "\n",
        "In general, we don't recommend using all available cores for parallelization:\n",
        "\n",
        "```python\n",
        "with Pool(processes=mp.cpu_count()) as pool:\n",
        "```\n",
        "\n",
        "This often results in slower execution, as the system spends time managing threads instead of doing actual computation. It's usually better to leave **one core free** for system processes:\n",
        "\n",
        "```python\n",
        "with Pool(processes=mp.cpu_count() - 1) as pool:\n",
        "```\n",
        "\n",
        "That said, Colab's VM only provides two cores, so `mp.cpu_count() - 1`  is pointless here. For now, let's just try a pathos batch run on 2 cores and compare the timing to the serial loop from earlier.\n",
        "\n",
        "> **Exercise 7**\n",
        ">\n",
        "> Test how different values of processes in the Pool() function affect your batch fitting time.\n",
        "> Try using 1, 2, and maybe `mp.cpu_count()` if you're on your local runtime.\n",
        "What's the fastest? What's the most efficient?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJAlYnRHGZAI",
        "outputId": "27d7b4e3-3034-4dc4-8daf-7960a18098bb"
      },
      "outputs": [],
      "source": [
        "# Main function to parallelize the processing\n",
        "def run_parallel_processing(plot_fraction=None, kill_after=None):\n",
        "    # numpy array for the fit params\n",
        "    fit_params = np.zeros((ews_df.shape[0], 8))\n",
        "\n",
        "    if plot_fraction is None:\n",
        "        plot_fraction = 0.1\n",
        "    if kill_after is None:\n",
        "        kill_after = 30\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create a pool of worker processes\n",
        "    print('CPU count:', mp.cpu_count())\n",
        "    with Pool(processes=mp.cpu_count()) as pool: # if running locally, keep the\n",
        "                                                 # process count < mp.cpu_count()\n",
        "                                                 # or it will be very slow\n",
        "        results = pool.map(lambda i: process_event(i, ews_df, int(1/plot_fraction), start_time), range(kill_after))\n",
        "        for i, params in results:\n",
        "            n = len(params)\n",
        "            fit_params[i, :n] = params\n",
        "\n",
        "    print(\"Total time:\", time.time() - start_time)\n",
        "\n",
        "    # Save fit_params if needed\n",
        "    # np.save('fit_params.npy', fit_params)\n",
        "\n",
        "    return fit_params\n",
        "\n",
        "# Run the parallel processing function\n",
        "######\n",
        "#fit_params = run_parallel_processing() # test run\n",
        "fit_params = run_parallel_processing(kill_after=N, plot_fraction=plot_fraction) # test run\n",
        "#fit_params = run_parallel_processing(kill_after=ews_df.shape[0],\n",
        "#                                     plot_fraction=0.01\n",
        "#                                     ) # full run\n",
        "######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQMhYxHAfIwT",
        "outputId": "0a1d5b93-5457-4682-ad38-292ffaa721bd"
      },
      "outputs": [],
      "source": [
        "fit_params[:N]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LCT3QifHGlWu",
        "outputId": "ae77344a-78d0-4795-930f-1b0aea09d0a1"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 7 fit parameter distributions\n",
        "\n",
        "fit5_params = fit_params[:N].copy()\n",
        "plot_histograms(fit5_params, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG0V-EU8HDGR"
      },
      "source": [
        "In beta testing, the timing results for fitting 30 events were:\n",
        "\n",
        "| Processing | Cores | Events (N) | Plots | Time |\n",
        "| :-: | :-: | :-: | :-: | :-: |\n",
        "| Serial | 1 | 30 | 100% | 246s |\n",
        "| Serial | 1 | 30 | 10% | 237s |\n",
        "| Parallel | 2 | 30 | 100% | 218s |\n",
        "| Parallel | 2 | 30 | 10% | 213s |\n",
        "\n",
        "A modest improvement, considering the extra coding overhead. However, on a local machine with more cores:\n",
        "\n",
        "| Processing | Cores | Events (N) | Plots | Time |\n",
        "| :-: | :-: | :-: | :-: | :-: |\n",
        "| Serial | 1 | 30 | 100% | 150s |\n",
        "| Serial | 1 | 30 | 10% | 144s |\n",
        "| Parallel | 7 | 30 | 100% | 56s |\n",
        "| Parallel | 7 | 30 | 10% |\t55s |\n",
        "\n",
        "\n",
        "This shows the real power of parallelisation—when the hardware can actually support it.\n",
        "\n",
        "If you even encounter this kind of problem in the wild, where you a very parallelisable job with limited time to implement parallelization, it's always worth considering poor-man parallelisation:\n",
        "Break the job into batches, run multiple scripts or terminals at once, and let your operating system juggle the rest.\n",
        "\n",
        "If your code is loop heavy, look for vectorisation opportunities using `numpy`.\n",
        "Or `cython`, if that's not an option.\n",
        "\n",
        "Now that you've seen how parallelisation affects efficiency, you're ready to run a full-season fit.\n",
        "However, if you're short on time, it's entirely reasonable to skip this exercise.\n",
        "\n",
        "> **Exercise 8**\n",
        ">\n",
        "> Use the more efficient method (serial or parallel) to process the full season of EWS data.\n",
        "> Plot histograms of the resulting parameter distributions (e.g., $t_\\textrm{E}$, $u_0$, etc.).\n",
        "> Look for trends. Look for outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wYysb_DpJwa9",
        "outputId": "4d7ab473-f284-45b9-ed86-d17c860357de"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 8 fit parameter distributions\n",
        "\n",
        "fit6_params = fit_params[:N].copy()\n",
        "plot_histograms(fit6_params, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzd3gcOPyW4H"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.4 Custom Event Finder </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPfkNPtHypE1"
      },
      "source": [
        "If we weren't borrowing initial fit parameters from the OGLE EWS, these fits would fail completely under downhill optimizers. Starting too far from the truth in parameter space means the optimizer gets trapped in local minima - the peaks and valleys of the likelihood landscape become a cage, not a guide.\n",
        "\n",
        "We'll explore more robust (and computationally expensive) methods in the next section. But even with advanced samplers, having a good initial guess makes the entire process faster, more stable, and more scientifically honest.\n",
        "\n",
        "Different ground-based surveys use different strategies to identify microlensing events in stellar light curves. You, however, have a luxury they don't: you already know which light curves contain events. That means your challenge isn't classification—it's localization.\n",
        "\n",
        "And the most critical parameter to localize is $t_0$.  \n",
        "Other parameters (like $u_0$, $t_\\textrm{E}$, and $\\rho$) can be guessed with population-based heuristics. But if your $t_0$ is wrong, your fit will miss the peak entirely - and what you get back will be noise-dressed nonsense.\n",
        "\n",
        "This is a central problem in bulk fitting single-lens microlensing events:  \n",
        "> We need an event-finding algorithm to guess $t_0$ accurately.\n",
        "\n",
        "You now have:\n",
        "- A full season of OGLE data\n",
        "- Light curves that are guaranteed to contain events\n",
        "- A set of academic papers describing common event-finding strategies:\n",
        "  - [OGLE Early Warnign System (EWS)](https://ui.adsabs.harvard.edu/abs/1994AcA....44..227U/abstract)\n",
        "  - [KMTNet Alert Finder](https://ui.adsabs.harvard.edu/abs/2018arXiv180607545K/abstract)\n",
        "  - [KMTNet Event Finder](https://ui.adsabs.harvard.edu/abs/2018AJ....155...76K/abstract)\n",
        "  - [KMTNet Anomaly Finder](https://ui.adsabs.harvard.edu/abs/2021AJ....162..163Z/abstract)\n",
        "  - [RTModel (Real-Time Modeling)](https://ui.adsabs.harvard.edu/abs/2024A%26A...688A..83B/abstract) (see their section 4)\n",
        "\n",
        "Do you think you can do better?\n",
        "\n",
        "> **Exercise 9**\n",
        ">\n",
        "> Write an algorithm to estimate $t_0$ for microlensing events.  \n",
        "> Keep it simple. Keep it efficient.  \n",
        "> The rest of your fit may depend on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki_q_-3xWjVO"
      },
      "outputs": [],
      "source": [
        "#@title Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwwlRkfxL-V5"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\">3.4 Advanced Modeling Techniques and Higher-Order Models</font>\n",
        "\n",
        "The idea that a good $t_0$ guess is enough for a good fit holds true for FSPL events. But when an FSPL model fails to fit cleanly, that's often a sign that higher-order effects are at play. You've encountered some of these in other chapters of this course.\n",
        "\n",
        "We know that binary stars are common in the galaxy. And yet, we often model events using PSPL or FSPL, as if single-lens-single-source events are the default. In fact, recent simulations of ground-based surveys using modern Galactic models suggest that around 50% of single-peaked microlensing events are actually \"hidden binaries\"—either:\n",
        "- **Binary lenses** (multiple objects in the lens system), or  \n",
        "- **Binary sources** (two source stars lensed simultaneously).\n",
        "\n",
        "#### <font face=\"Helvetica\" size=\"4\"> Binary source stars </font>\n",
        "They introduce subtle distortions to the light curve and can easily masquerade as single-lens, binary-lens events.  \n",
        "\n",
        "If you'd like to learn more, see this [notebook on binary sources](https://github.com/AmberLee2427/TheMicrolensersGuideToTheGalaxy/blob/main/Notebooks/BinarySource.ipynb).\n",
        "\n",
        "#### <font face=\"Helvetica\" size=\"4\"> Binary lenses </font>\n",
        "These are a different beast entirely. They require:\n",
        "- Higher-order models\n",
        "- Optimizers that can escape local minima (e.g. **Monte Carlo** methods that allow uphill steps)\n",
        "\n",
        "  ([This notebook](https://github.com/AmberLee2427/TheMicrolensersGuideToTheGalaxy/blob/main/Notebooks/Modelling.ipynb) on modelling methods commonly used in microlensing may also be of interest)\n",
        "- Often a **grid search** in $s$, $q$, and $\\rho$ to even get reasonable initial conditions\n",
        "\n",
        "Even with that, **degeneracies are common** — and easy to miss. For example, see the modeling of OGLE-2016-BLG-1195 (Shartzvald et al., 2017; Bond et al., 2017; Gould et al., 2023; Vandorou et al., 2024), where a viable degenerate solutions were overlooked at each stage of modelling.\n",
        "\n",
        "If a light curve shows dramatic deviations from a Paczyński shape, you're **almost certainly dealing with a binary lens** (unless some other astrophysical event has contaminated your lightcurve). But determining the *correct* model requires balancing:\n",
        "- Evidence for complexity, and  \n",
        "- The principle of parsimony (Occam's Razor), while knowing full well that these “complex” models are not **rare.**\n",
        "\n",
        "---\n",
        "\n",
        "For more on Bayesian modeling and MC methods in microlensing, see our [modeling notebook](https://github.com/AmberLee2427/TheMicrolensersGuideToTheGalaxy/blob/main/Notebooks/Modelling.ipynb).\n",
        "\n",
        "---\n",
        "\n",
        "#### Other higher-order effects not covered in this course (but worth knowing):\n",
        "- **Lens orbital motion**  \n",
        "- **Xallarap** (source orbital motion)  \n",
        "- **Multiple lenses** (e.g. triple lenses)  \n",
        "- **Variable stars** (source, lens, or blend stars with intrinsic variability)  \n",
        "- Variable **blending** (ambient stars moving in/out of the aperture/PSF)  \n",
        "- General data **systematics**  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3MbPK5SLH11"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> 4. Full Season *Roman* Fit </font>\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "First, the data. There is a repo full of *Roman*-like light curves from the 2018 WFIRST Data Challenge. We start our mini data challenge by cloning that and pulling out all the relavent lightcurves. Feel free to blindly `SHFT` + `ENTR` your way through this part until you get to Section 4.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7R1s5sXVW4T"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 4.1 Getting the data </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BZXAP0iB7gR9",
        "outputId": "857dd63b-f3f0-4145-82fd-720967ffb9bb"
      },
      "outputs": [],
      "source": [
        "#@title Cloning the GitHub repository\n",
        "\n",
        "# clone the microlensing data challenge repo\n",
        "!git clone https://github.com/microlensing-data-challenge/data-challenge-1.git\n",
        "\n",
        "# Extract the lightcurve files\n",
        "!tar -xzvf data-challenge-1/lc.tar.gz -C data-challenge-1/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvEn1f-C-mlf"
      },
      "outputs": [],
      "source": [
        "#@title Displaying PDFs in a notebook (browser dependent compatability)\n",
        "#from IPython.display import IFrame\n",
        "#\n",
        "## Assuming the PDF is in the current working directory\n",
        "#pdf_path = \"data-challenge-1/Answers/DataChallenge2019_Summary_byJenniferYee.pdf\"\n",
        "#\n",
        "## Display the PDF using IFrame\n",
        "#IFrame(pdf_path, width=800, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aYEHo7-QOgI"
      },
      "source": [
        "This dataset includes 293 lightcurve, 74 of which are single lens events. We can cheat a little and specifically pull out the events that we know to be single lenses, keeping the challenge tractable for completion within the hour, with the added benefit of making the strangley organized `master_file.txt` easier to wrangle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "UDJXNqxQ_WzH",
        "outputId": "f8259b94-d9e1-40f3-b6b2-0ba069b9969f"
      },
      "outputs": [],
      "source": [
        "#@title Putting everything in a tidy data frame\n",
        "\n",
        "master_file = '/content/data-challenge-1/Answers/master_file.txt'\n",
        "header_file = '/content/data-challenge-1/Answers/wfirstColumnNumbers.txt'\n",
        "\n",
        "rows = []\n",
        "with open(master_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        # Skip empty lines or comment lines\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "\n",
        "        tokens = line.split()  # split on whitespace\n",
        "        # Keep only single-lens events\n",
        "        if \"dcnormffp\" not in tokens:\n",
        "            continue\n",
        "\n",
        "        # Single-lens lines should have exactly 96 columns\n",
        "        if len(tokens) != 96:\n",
        "            continue\n",
        "\n",
        "        rows.append(tokens)\n",
        "\n",
        "df_sl = pd.DataFrame(rows)\n",
        "\n",
        "# make an array of zeros with 97 elements\n",
        "colnames_96 = np.zeros(96, dtype=object)\n",
        "\n",
        "# Read the header file\n",
        "with open(header_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        # Skip empty lines or comments\n",
        "        if not line or line.startswith('#'):\n",
        "            continue\n",
        "        # The second token is the 'name'\n",
        "        parts = line.split()\n",
        "        colnames_96[int(parts[0])] = parts[1]\n",
        "\n",
        "#For single lenses they are (***Note for these, the mass of the lens is given by the planet mass column, not the host mass column):\n",
        "#72 - unimportant\n",
        "#73 - N, number of consecutive W149 data points deviating by >=3 sigma from a flat line\n",
        "#74 - unimportant\n",
        "#75 - Delta chi^2 (relative to a flat line)\n",
        "#76-91 - unimportant\n",
        "#92 - simulated event type (dcnormffp = single lens or free-floating planet)\n",
        "#93 - unimportant (I think)\n",
        "#94 - lightcurve filename root\n",
        "#95 - Data challenge lightcurve number\n",
        "\n",
        "# Replace the column names in colnames_96\n",
        "colnames_96[73] = 'N'\n",
        "colnames_96[75] = 'Delta chi2'\n",
        "colnames_96[92] = 'sim type'\n",
        "colnames_96[94] = 'filename'\n",
        "colnames_96[95] = 'lc_number'\n",
        "\n",
        "# Make sure the column names are unique\n",
        "for i in range(94):\n",
        "    if colnames_96[i] == '|' or colnames_96[i] == 0:\n",
        "        colnames_96[i] = 'col_' + str(i)\n",
        "\n",
        "# Replace the column names in the data_frame\n",
        "df_sl.columns = colnames_96\n",
        "\n",
        "# Remove the dummy columns 'col_*'\n",
        "df_sl = df_sl.loc[:, ~df_sl.columns.str.startswith('col_')]\n",
        "\n",
        "df_sl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubuyz4fM-X78"
      },
      "source": [
        "The last column in this data frame has the lightcurve number, which we can use to pick out only the lightcurves matching our single-lens event list, for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "33wo759fRxfb",
        "outputId": "1ac6d3e1-c0ea-474e-c6dd-66eb197ebef8"
      },
      "outputs": [],
      "source": [
        "#@title Figuring out which files we want\n",
        "\n",
        "lc_number = df_sl['lc_number'].to_numpy()\n",
        "\n",
        "lc_file_path_format = 'data-challenge-1/lc/ulwdc1_XXX_filter.txt'\n",
        "\n",
        "lc_file_paths_W149 = [lc_file_path_format.replace('filter', 'W149')] * len(lc_number)\n",
        "lc_file_paths_Z087 = [lc_file_path_format.replace('filter', 'Z087')] * len(lc_number)\n",
        "\n",
        "# replace XXX, from the right, with the lc_number which is not necessarily of length 3\n",
        "lc_file_paths_W149 = [path.replace('XXX', str(num).zfill(3)) for path, num in zip(lc_file_paths_W149, lc_number)]\n",
        "lc_file_paths_Z087 = [path.replace('XXX', str(num).zfill(3)) for path, num in zip(lc_file_paths_Z087, lc_number)]\n",
        "\n",
        "df_sl['lc_file_path_W149'] = lc_file_paths_W149\n",
        "df_sl['lc_file_path_Z087'] = lc_file_paths_Z087\n",
        "\n",
        "df_sl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlIxCof6VY9o"
      },
      "source": [
        "There are a few pieces of information that may need to be known for each event that are not in the lightcurve files. These are stored in event_info.txt\n",
        "\n",
        "Columns: `\"Event_name\"` `\"Event_number\"` `\"RA_(deg)\"` `\"Dec_(deg)\"` `\"Distance\"` `\"A_W149\"` `\"sigma_A_W149\"` `\"A_Z087\"` `\"sigma_A_Z087\"`\n",
        "\n",
        "Distance, A_W149/Z087 are an estimate of the distance and extinction in each band of the red clump stars. sigma_A_W149/Z087 are dispersions in the extinction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "ke7lpciNVtEN",
        "outputId": "b0840ec4-6759-4af6-ab04-ef47df25585d"
      },
      "outputs": [],
      "source": [
        "#@title Event information data frame\n",
        "\n",
        "header = [\"Event_name\",\n",
        "          \"Event_number\",\n",
        "          \"RA_(deg)\",\n",
        "          \"Dec_(deg)\",\n",
        "          \"Distance\",\n",
        "          \"A_W149\",\n",
        "          \"sigma_A_W149\",\n",
        "          \"A_Z087\",\n",
        "          \"sigma_A_Z087\"\n",
        "]\n",
        "\n",
        "event_info = pd.read_csv('./data-challenge-1/event_info.txt', names=header, delim_whitespace=True)\n",
        "event_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "H_wJ09k8W1Hr",
        "outputId": "13c370ee-d47e-409a-919e-821000e11be3"
      },
      "outputs": [],
      "source": [
        "#@title Combining the two data frames\n",
        "\n",
        "# Convert 'lc_number' to numeric type before merging\n",
        "merged_df = pd.merge(event_info, df_sl.astype({'lc_number': 'int64'}), left_on='Event_number', right_on='lc_number', how='inner')\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "wkRfC7W26Akn",
        "outputId": "bc5c7dc8-96d6-4214-8634-0db94463585b"
      },
      "outputs": [],
      "source": [
        "try:  # this will only work on Colab.\n",
        "  sheet = sheets.InteractiveSheet(df=merged_df)\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPfdr0mJ7fOb"
      },
      "source": [
        "Great - data successfully wrangled. Let's forget we ever had to live through that and move right along.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> 4.2 Adjusting the model for L2 an orbit </font>\n",
        "\n",
        "Next, we need to make a minor adjustment to our model for data that is not ground based. In `MulensModel` this simply means adding the following keyword to the data object initialization: `ephemerides_file=PATH_TO_THE_FILE`.\n",
        "\n",
        "> Instructions specific to this data set for `MulensModel` are given [here](https://github.com/rpoleski/MulensModel/blob/master/documents/data_challenge.md).\n",
        "\n",
        "Most of the data for these events is in the W147 band, so we make the very reasonable decision to just fit those data and not have to deal with mutliple data sets with different $F_\\textrm{S}$ and $F_\\textrm{B}$ values. That should speen up our fits too. If we wanted to find the color of the source star at a later date we could fit just the flux parameters and leave the microlensing-model parameters fixed (as described in [this notebook](https://github.com/AmberLee2427/TheMicrolensersGuideToTheGalaxy/blob/3b783495eb9a916ee9670a0347c9325f6a5b0a21/Notebooks/SingleLens.ipynb)) using a linear regression, which would a fraction of a second per event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLvFYejBLQFY",
        "outputId": "adc8d511-817f-47cb-f6ab-4c0d06c28d97"
      },
      "outputs": [],
      "source": [
        "#@title Example L2 data object\n",
        "\n",
        "data_file = merged_df['lc_file_path_W149'][0]\n",
        "t_0 = float(merged_df['t0'][0])\n",
        "# split the parallax equally out of a lack of better ideas\n",
        "pi_E_E = np.sqrt(float(merged_df['piE'][0])**2 / 2.0)\n",
        "pi_E_N = pi_E_E * 1.0\n",
        "ra = merged_df['RA_(deg)'][0]\n",
        "dec = merged_df['Dec_(deg)'][0]\n",
        "print(ra, dec)\n",
        "\n",
        "# convert decimal ra and dec in degrees to \"17h57m16.56s -29d05m20.04s\"\n",
        "coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame='icrs')\n",
        "hms_dms_string = coord.to_string('hmsdms')\n",
        "print(f\"SkyCoord default: {hms_dms_string}\")\n",
        "\n",
        "# Here is the main difference - we provide the ephemeris for Roman:\n",
        "EPHEM_FILE = 'data-challenge-1/wfirst_ephemeris_W149.txt'\n",
        "data_Roman_W149 = MulensModel.MulensData(file_name=data_file,\n",
        "                                         phot_fmt='mag',\n",
        "                                         ephemerides_file=EPHEM_FILE,\n",
        "                                         plot_properties={'color': '#a859e4',\n",
        "                                                          'label': 'Roman W149'\n",
        "                                                          },\n",
        "                                         bandpass='H'\n",
        "                                         )\n",
        "\n",
        "# Annoyingly, t_0 is in simulation time not HJD to we need to do an approximate\n",
        "# conversion\n",
        "# (https://github.com/microlensing-data-challenge/evaluation_code/blob/master/parse_table1.py)\n",
        "# line 402\n",
        "t_0 = t_0 + 2458234.0  # simulation 0 time\n",
        "\n",
        "\n",
        "# Let's just tidy the \"guess\" parameters up into a dictionary, for easy accesss\n",
        "params = dict()\n",
        "parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\", \"rho\", \"pi_E_N\", \"pi_E_E\"]\n",
        "params['t_0'] = t_0 * 1.0\n",
        "params['t_0_par'] = t_0 * 1.0\n",
        "params['u_0'] = float(merged_df['u0'][0]) * 1.1\n",
        "params['t_E'] = float(merged_df['tE'][0]) * 1.1\n",
        "params['rho'] = float(merged_df['rhos'][0]) * 1.1\n",
        "params['pi_E_N'] = pi_E_E\n",
        "params['pi_E_E'] = pi_E_E\n",
        "\n",
        "# If we are using parallax, it is also important that we provide the event\n",
        "# coordinates, or MulensModel can't do necessary calculations\n",
        "Roman_model = MulensModel.Model({**params},\n",
        "                                coords=coord,\n",
        "                                ephemerides_file=EPHEM_FILE\n",
        "                                )\n",
        "\n",
        "Roman_event = MulensModel.Event(datasets=data_Roman_W149, model=Roman_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV7HUjMcyRVX",
        "outputId": "ca717797-df72-4e0b-dadf-18fc9b9edfba"
      },
      "outputs": [],
      "source": [
        "#@title Sneaky peak at the data\n",
        "\n",
        "print(' ', data_file, '\\n')\n",
        "! head -5 data-challenge-1/lc/ulwdc1_005_W149.txt\n",
        "\n",
        "print('\\n', EPHEM_FILE)\n",
        "! head -5 data-challenge-1/wfirst_ephemeris_W149.txt\n",
        "\n",
        "print('\\n Object lightcurve dates')\n",
        "print(data_Roman_W149.time)\n",
        "\n",
        "print('\\n t_0:', params['t_0'])\n",
        "print(' t_0_par:', params['t_0_par'])\n",
        "\n",
        "# cool, everything is in full HJD and looking sensible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "dUG01D7s4zeh",
        "outputId": "a65137fb-fcb7-4117-9fd4-96ebfc635ecf"
      },
      "outputs": [],
      "source": [
        "#@title Plot to check it's working\n",
        "Roman_event = MulensModel.Event(datasets=data_Roman_W149, model=Roman_model)\n",
        "\n",
        "t_min = params['t_0'] - 3.0 * params['t_E']\n",
        "t_max = params['t_0'] + 3.0 * params['t_E']\n",
        "print(t_min, t_max)\n",
        "print(t_0)\n",
        "\n",
        "# set finite source method\n",
        "Roman_model.set_magnification_methods([t_min,\n",
        "                                       finite_source_methods[0],\n",
        "                                       t_max],\n",
        "                                      source=None\n",
        "                                      )  # rho <= 0.1\n",
        "\n",
        "plt.close(200)\n",
        "plt.figure(200)\n",
        "\n",
        "#Roman_model.plot_magnification(t_range=[t_min, t_max],\n",
        "#                                    subtract_2450000=True,\n",
        "#                                    color='red',\n",
        "#                                    linestyle=':'\n",
        "#                                    )\n",
        "#Roman_event.plot_data(subtract_2450000=True)\n",
        "Roman_event.plot()\n",
        "\n",
        "plt.xlim(t_min-2450000, t_max-2450000)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4hlbXoorffu"
      },
      "source": [
        "Yeah, cool cool. Not great. But good enough for a starting guess, if that's the route you're taking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDj1BFoDHCOj"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 4.3 Do it </font>\n",
        "\n",
        "This is it. You have every thing you need to fit a full season of single lenses. And this is the part where we push the baby bird out of the nest. No more hand-holding. No sample answers. Just do it.\n",
        "\n",
        "I believe in you.\n",
        "\n",
        "> **Exercise 10**\n",
        ">\n",
        "> Perform FSPL on the provided simulated Roman single-lens events.\n",
        ">\n",
        "> *Note. Don't forget to save your best fit parameters for later inspection and to add parallax to the model.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wVjw47SzHBmf"
      },
      "outputs": [],
      "source": [
        "#@title Your code goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zIIH2x4MLZR0",
        "outputId": "d9ba42b4-3e57-4555-92b5-3fa66a6eca46"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 10 fit parameter distributions\n",
        "\n",
        "fit7_params = fit_params[:N].copy()\n",
        "plot_histograms(fit7_params, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwg-froHeQm"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 4.4 How did you do? </font>\n",
        "\n",
        "The next step is evalutaing how well your bulk fit went. Some simple histograms of fitted parameters compared with \"truths\" should do the trick.\n",
        "\n",
        "> **Exercise 11**\n",
        ">\n",
        "> Make overlayed histograms of the true parameters distributions and your best-fit parameter distributions to evaluate the sucess of your algoryhthms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA8-nRnMIS_U"
      },
      "outputs": [],
      "source": [
        "#@title Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi-RJ_TRLad4"
      },
      "source": [
        "<!-- ## <font face=\"Helvetica\" size=\"6\"> 5. Meta Analysis </font>*italicized text*\n",
        "<hr style=\"border: 1.5pt solid #fc3d21; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "Your full season fit is probably going to take some serious timme to finish, so we have included this section purely as a homework exercise. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUrW-CXmIVY5"
      },
      "source": [
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "Well done for completing this notebook. If you enjoyed the challenge or feel like you would like to try to improve your approach--if you think you can do better than the current microlensing fitting approaches--look out for the Roman Microlensing Data Challenge comming out in September."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP+TSchZRCAfwCcW+DZDMDV",
      "mount_file_id": "1dK339mAVNtIqC8WDUS8FfVj6K_a3z_jO",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
