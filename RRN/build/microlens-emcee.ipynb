{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Microlensing MCMC Analysis Module\n",
    "\n",
    "This module provides a class-based interface for performing MCMC analysis\n",
    "on microlensing models using the emcee package.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import emcee\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocess import Pool\n",
    "from scipy.optimize import lsq_linear\n",
    "import VBMicrolensing as vbm\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# External Model Functions (can be overridden by user)\n",
    "# ============================================================================\n",
    "\n",
    "def get_mag(x0, p, t):\n",
    "    '''\n",
    "    Get the magnification curve for a given model and parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x0: np.ndarray with initial parameters (t0, tE, u0, rho, s, q, alpha)\n",
    "    p: list[str] with parameter names\n",
    "    t: np.ndarray with data epochs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mag: np.ndarray with magnification curve\n",
    "    '''\n",
    "    # We will use the VBMicrolensing package to calculate magnifications for different source positions.\n",
    "    VBM = vbm.VBMicrolensing()\n",
    "     #create a dictionary of parameters\n",
    "    params = dict(zip(p, x0))\n",
    "    paramsc = params.copy()\n",
    "    tau = (t - paramsc['t0']) / paramsc['tE'] \n",
    "    # Single lens model\n",
    "    if len(p) < 7 :\n",
    "        ul = np.sqrt(tau**2 + paramsc['u0']**2)\n",
    "        if 'logrho' in paramsc.keys():\n",
    "            paramsc['rho'] = 10**(paramsc['logrho'])\n",
    "        mag = np.array([VBM.ESPLMag2(u, paramsc['rho']) for u in ul])\n",
    "    # Binary lens model\n",
    "    elif len(p) >= 7:\n",
    "        salpha = np.sin(np.radians(paramsc['alpha']))\n",
    "        calpha = np.cos(np.radians(paramsc['alpha']))\n",
    "        xs = -paramsc['u0'] * salpha + tau * calpha \n",
    "        ys = paramsc['u0'] * calpha + tau * salpha\n",
    "        if 'logs' in paramsc.keys():\n",
    "            paramsc['s'] = 10**(paramsc['logs'])\n",
    "        if 'logq' in paramsc.keys():\n",
    "            paramsc['q'] = 10**(paramsc['logq'])\n",
    "        if 'logrho' in paramsc.keys():\n",
    "            paramsc['rho'] = 10**(paramsc['logrho'])\n",
    "        mag = np.array([VBM.BinaryMag2(paramsc['s'], paramsc['q'], xs[i], ys[i], paramsc['rho']) for i in range(len(ys))])\n",
    "    return mag\n",
    "\n",
    "\n",
    "def calc_Fs(modelmag: np.ndarray, f: np.ndarray, sig2: np.ndarray) -> tuple[float, float]:\n",
    "    '''\n",
    "    Solves for the flux parameters for a given model using least squares.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : np.ndarray\n",
    "        Model magnification curve.\n",
    "    f : np.ndarray\n",
    "        Observed flux values.\n",
    "    sig2 : np.ndarray\n",
    "        Flux errors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    FS : float\n",
    "        Source flux.\n",
    "    FB : float\n",
    "        Blend flux.\n",
    "    '''\n",
    "\n",
    "    \"\"\"\n",
    "    Solves for FS, FB by weighted least squares with FB >= 0 using lsq_linear.\n",
    "    \"\"\"\n",
    "    modelmag = np.asarray(modelmag, float).ravel()\n",
    "    f = np.asarray(f, float).ravel()\n",
    "    sig2 = np.asarray(sig2, float).ravel()\n",
    "\n",
    "    w = 1.0 / np.sqrt(sig2)\n",
    "    A = np.column_stack([modelmag, np.ones_like(modelmag)])\n",
    "    Aw = A * w.reshape(-1, 1)\n",
    "    bw = f * w\n",
    "\n",
    "    res = lsq_linear(Aw, bw)\n",
    "    FS, FB = res.x\n",
    "    return float(FS), float(FB)\n",
    "\n",
    "\n",
    "def chi2(x0: np.ndarray, p: list, t: np.ndarray, f: np.ndarray, sig: np.ndarray) -> float:\n",
    "    '''\n",
    "    Calculates the chi squared value for a given model and parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x0 : np.ndarray\n",
    "        Initial parameters.\n",
    "    p : List[str]\n",
    "        List of parameter names.\n",
    "    t : np.ndarray\n",
    "        Data epochs.\n",
    "    f : np.ndarray\n",
    "        Observed flux values.\n",
    "    sig : np.ndarray\n",
    "        Flux errors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    chi2 : float\n",
    "        Chi squared value.\n",
    "   '''\n",
    "    mag = get_mag(x0, p, t)\n",
    "    FS, FB = calc_Fs(mag, f, sig**2)\n",
    "    model = FS * mag + FB\n",
    "    \n",
    "    chi2_value = np.sum((f - model)**2 / sig**2)\n",
    "    return chi2_value\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main MCMC Analysis Class\n",
    "# ============================================================================\n",
    "\n",
    "class micro_mc:\n",
    "    \"\"\"\n",
    "    A class for performing MCMC analysis on microlensing models.\n",
    "    \n",
    "    This class encapsulates all functionality needed to fit microlensing\n",
    "    light curves using MCMC methods, including convergence diagnostics\n",
    "    and visualization tools.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with data epochs, flux, and flux errors.\n",
    "        Expected columns: 'HJD-2450000', 'I_band_flux', 'I_band_flux_err'\n",
    "    x0 : np.ndarray\n",
    "        Initial parameter values\n",
    "    param_names : list[str]\n",
    "        List of parameter names\n",
    "    log_probability : callable\n",
    "        User-provided log probability function with signature:\n",
    "        log_probability(x, p, t, f, sig, bounds) -> float\n",
    "    bounds : dict | None, optional\n",
    "        Optional map of parameter name -> (min, max). If None, use defaults.\n",
    "    fixed_params : dict | None, optional\n",
    "        Parameters to freeze as {name: value}. Only other parameters are sampled.\n",
    "    get_mag_func : callable | None, optional\n",
    "        Custom magnification function. If None, uses default get_mag.\n",
    "    calc_Fs_func : callable | None, optional\n",
    "        Custom flux calculation function. If None, uses default calc_Fs.\n",
    "    chi2_func : callable | None, optional\n",
    "        Custom chi-squared function. If None, uses default chi2.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    results : dict\n",
    "        Dictionary containing MCMC analysis results (set after running perform_mcmc_analysis)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, x0, param_names, log_probability, \n",
    "                 bounds=None, fixed_params=None,\n",
    "                 get_mag_func=None, calc_Fs_func=None, chi2_func=None):\n",
    "        \"\"\"Initialize the micro_mc analysis class.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with data epochs, flux, and flux errors.\n",
    "        Expected columns: 'HJD-2450000', 'I_band_flux', 'I_band_flux_err'\n",
    "    x0 : np.ndarray\n",
    "        Initial parameter values\n",
    "    param_names : list[str]\n",
    "        List of parameter names\n",
    "    log_probability : callable\n",
    "        User-provided log probability function with signature:\n",
    "        log_probability(x, p, t, f, sig, bounds) -> float\n",
    "    bounds : dict | None, optional\n",
    "        Optional map of parameter name -> (min, max). If None, use defaults.\n",
    "    fixed_params : dict | None, optional\n",
    "        Parameters to freeze as {name: value}. Only other parameters are sampled.\n",
    "    get_mag_func : callable | None, optional\n",
    "        Custom magnification function. If None, uses default get_mag.\n",
    "    calc_Fs_func : callable | None, optional\n",
    "        Custom flux calculation function. If None, uses default calc_Fs.\n",
    "    chi2_func : callable | None, optional\n",
    "        Custom chi-squared function. If None, uses default chi2.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    results : dict\n",
    "        Dictionary containing MCMC analysis results (set after running perform_mcmc_analysis)\n",
    "    \"\"\"\n",
    "        self.df = df\n",
    "        self.x0 = x0\n",
    "        self.param_names = param_names\n",
    "        self.log_probability = log_probability\n",
    "        self.bounds = bounds\n",
    "        self.fixed_params = fixed_params\n",
    "        \n",
    "        # Store function references (use defaults if not provided)\n",
    "        self._get_mag_func = get_mag_func if get_mag_func is not None else get_mag\n",
    "        self._calc_Fs_func = calc_Fs_func if calc_Fs_func is not None else calc_Fs\n",
    "        self._chi2_func = chi2_func if chi2_func is not None else chi2\n",
    "        \n",
    "        # Results will be stored here after running analysis\n",
    "        self.results = None\n",
    "    \n",
    "    def run_mcmc(self, steps=500, walkers=100, step_scale=1e-4, \n",
    "                 param_scales=None, verbose=False, pool=None):\n",
    "        \"\"\"\n",
    "        Fit an \"event\" with \"parameters_to_fit\" as free parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int, optional\n",
    "            Number of MCMC steps (default: 500)\n",
    "        walkers : int, optional\n",
    "            Number of walkers (default: 100)\n",
    "        step_scale : float, optional\n",
    "            Global step size scaling factor (default: 1e-4)\n",
    "        param_scales : dict[str, float] | np.ndarray | None, optional\n",
    "            Either dict of per-parameter step sizes or array in p-order.\n",
    "        verbose : bool, optional\n",
    "            Print step size information (default: False)\n",
    "        pool : multiprocessing.Pool | None, optional\n",
    "            Multiprocessing pool for parallel evaluation of log probability across walkers.\n",
    "            If None, runs in serial mode. (default: None)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        sampler, pos, prob, state, free_param_names\n",
    "            MCMC sampler results\n",
    "        \"\"\"\n",
    "        # Take the initial starting point from the event.\n",
    "        t = np.array(self.df['HJD-2450000'])\n",
    "        f = np.array(self.df['I_band_flux'])\n",
    "        sig = np.array(self.df['I_band_flux_err'])\n",
    "\n",
    "        # Default bounds for clipping initial positions\n",
    "        default_bounds = {\n",
    "            't0': (t.min(), t.max()),\n",
    "            'tE': (0.1, t.max() - t.min()),\n",
    "            'u0': (-10.0, 10.0),\n",
    "            'rho': (1e-5, 1.0),\n",
    "            's': (1e-2, 20.0),\n",
    "            'q': (1e-7, 1.0),\n",
    "            'alpha': (0.0, 360.0),\n",
    "            'logs': (-2, 2),\n",
    "            'logq': (-7, 0),\n",
    "            'logrho': (-5, 0),\n",
    "        }\n",
    "        bounds_map = default_bounds.copy()\n",
    "        if self.bounds is not None:\n",
    "            bounds_map.update({k: tuple(v) for k, v in self.bounds.items() if k in bounds_map})\n",
    "\n",
    "        fixed_params = self.fixed_params or {}\n",
    "        fixed_params = {k: float(v) for k, v in fixed_params.items()}\n",
    "\n",
    "        # free/fixed split\n",
    "        free_param_names = [name for name in self.param_names if name not in fixed_params]\n",
    "        ndim = len(free_param_names)\n",
    "        if ndim == 0:\n",
    "            raise ValueError(\"No free parameters to sample. Provide some free parameters or omit fixed_params.\")\n",
    "\n",
    "        # Build x0 map and free starting positions\n",
    "        x0_map = dict(zip(self.param_names, self.x0))\n",
    "        x0_free = np.array([x0_map[name] for name in free_param_names])\n",
    "\n",
    "        # Step sizes dict -> array for free params\n",
    "        if isinstance(param_scales, dict):\n",
    "            scales_free = np.array([param_scales.get(name, step_scale) for name in free_param_names])\n",
    "            if verbose:\n",
    "                print(\"Using dict step sizes for free parameters:\")\n",
    "                for name, sc in zip(free_param_names, scales_free):\n",
    "                    print(f\"  {name}: {sc:.2e}\")\n",
    "        elif isinstance(param_scales, np.ndarray):\n",
    "            if len(param_scales) != len(self.param_names):\n",
    "                raise ValueError(f\"param_scales array must match param_names length ({len(self.param_names)}).\")\n",
    "            name_to_scale = dict(zip(self.param_names, param_scales))\n",
    "            scales_free = np.array([name_to_scale[name] for name in free_param_names])\n",
    "            if verbose:\n",
    "                print(\"Using array step sizes mapped to free parameters:\")\n",
    "                for name, sc in zip(free_param_names, scales_free):\n",
    "                    print(f\"  {name}: {sc:.2e}\")\n",
    "        elif param_scales is None:\n",
    "            scales_free = np.array([step_scale] * ndim)\n",
    "            if verbose:\n",
    "                print(f\"Using global step size: {step_scale:.2e}\")\n",
    "        else:\n",
    "            raise ValueError(\"param_scales must be a dict, numpy array, or None\")\n",
    "\n",
    "        # Create initial starting points for all walkers in free space\n",
    "        nwalkers = walkers\n",
    "        p0state_free = x0_free + scales_free * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "        # Clip initial positions to within bounds for free params\n",
    "        for j, name in enumerate(free_param_names):\n",
    "            lower, upper = bounds_map.get(name, (-np.inf, np.inf))\n",
    "            p0state_free[:, j] = np.clip(p0state_free[:, j], lower, upper)\n",
    "\n",
    "        # Log prob wrapper that reconstructs full vector\n",
    "        def log_prob_free(theta_free: np.ndarray) -> float:\n",
    "            full_map = {**x0_map, **fixed_params}\n",
    "            for j, name in enumerate(free_param_names):\n",
    "                full_map[name] = float(theta_free[j])\n",
    "            x_full = np.array([full_map[name] for name in self.param_names])\n",
    "            return self.log_probability(x_full, self.param_names, t, f, sig, self.bounds)\n",
    "\n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob_free, pool=pool)\n",
    "\n",
    "        # Run MCMC\n",
    "        pos, prob, state = sampler.run_mcmc(p0state_free, steps, progress=True)\n",
    "\n",
    "        return sampler, pos, prob, state, free_param_names\n",
    "\n",
    "    @staticmethod\n",
    "    def gelman_rubin_statistic(chains):\n",
    "        \"\"\"Calculate Gelman-Rubin R-hat statistic for convergence assessment\"\"\"\n",
    "        n_walkers, n_steps, n_params = chains.shape\n",
    "        \n",
    "        if n_steps < 4:\n",
    "            return np.full(n_params, np.nan)\n",
    "        \n",
    "        # Split each chain in half\n",
    "        half_steps = n_steps // 2\n",
    "        chains_split = chains[:, -2*half_steps:, :].reshape(2*n_walkers, half_steps, n_params)\n",
    "        \n",
    "        r_hat = np.zeros(n_params)\n",
    "        \n",
    "        for i in range(n_params):\n",
    "            chain_means = np.mean(chains_split[:, :, i], axis=1)\n",
    "            chain_vars = np.var(chains_split[:, :, i], axis=1, ddof=1)\n",
    "            \n",
    "            overall_mean = np.mean(chain_means)\n",
    "            between_var = half_steps * np.var(chain_means, ddof=1)\n",
    "            within_var = np.mean(chain_vars)\n",
    "            \n",
    "            var_estimate = ((half_steps - 1) * within_var + between_var) / half_steps\n",
    "            r_hat[i] = np.sqrt(var_estimate / within_var) if within_var > 0 else 1.0\n",
    "        \n",
    "        return r_hat\n",
    "\n",
    "    @staticmethod\n",
    "    def effective_sample_size(chains):\n",
    "        \"\"\"Estimate effective sample size\"\"\"\n",
    "        n_walkers, n_steps, n_params = chains.shape\n",
    "        eff_sizes = np.zeros(n_params)\n",
    "        \n",
    "        for i in range(n_params):\n",
    "            # Flatten chains for this parameter\n",
    "            flat_chain = chains[:, :, i].flatten()\n",
    "            \n",
    "            # Simple autocorrelation-based estimate\n",
    "            autocorr = np.correlate(flat_chain - np.mean(flat_chain), \n",
    "                                   flat_chain - np.mean(flat_chain), mode='full')\n",
    "            autocorr = autocorr[autocorr.size // 2:]\n",
    "            autocorr = autocorr / autocorr[0]\n",
    "            \n",
    "            # Find where autocorr drops below 1/e\n",
    "            try:\n",
    "                cutoff = np.where(autocorr < 1/np.e)[0][0]\n",
    "                eff_sizes[i] = len(flat_chain) / (2 * cutoff + 1)\n",
    "            except IndexError:\n",
    "                eff_sizes[i] = len(flat_chain) / 10  # Conservative estimate\n",
    "        \n",
    "        return eff_sizes\n",
    "\n",
    "    def plot_corner_mcmc(self, samples=None, param_names=None, title=\"MCMC Posterior Distributions\"):\n",
    "        \"\"\"\n",
    "        Plot corner plot of MCMC samples\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : np.ndarray, optional\n",
    "            MCMC samples. If None, uses self.results['samples']\n",
    "        param_names : list, optional\n",
    "            Parameter names. If None, uses self.results['free_param_names']\n",
    "        title : str, optional\n",
    "            Plot title\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The corner plot figure\n",
    "        \"\"\"\n",
    "        if samples is None:\n",
    "            if self.results is None:\n",
    "                raise ValueError(\"No results available. Run perform_mcmc_analysis first.\")\n",
    "            samples = self.results['samples']\n",
    "            param_names = self.results['free_param_names']\n",
    "        \n",
    "        fig = corner.corner(samples, labels=param_names, \n",
    "                           quantiles=[0.16, 0.5, 0.84],\n",
    "                           show_titles=True, title_kwargs={\"fontsize\": 12},\n",
    "                           label_kwargs={\"fontsize\": 14})\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16, y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def plot_mcmc_fit(self, mle_params=None, param_names=None, mle_delta_chi2=None):\n",
    "        \"\"\"\n",
    "        Plot the best fit model from MCMC\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mle_params : np.ndarray, optional\n",
    "            MLE parameter values. If None, uses self.results['mle_params']\n",
    "        param_names : list, optional\n",
    "            Parameter names. If None, uses self.param_names\n",
    "        mle_delta_chi2 : float, optional\n",
    "            Delta chi-squared value. If None, uses self.results['mle_delta_chi2']\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The fit plot figure\n",
    "        \"\"\"\n",
    "        if mle_params is None:\n",
    "            if self.results is None:\n",
    "                raise ValueError(\"No results available. Run perform_mcmc_analysis first.\")\n",
    "            mle_params = self.results['mle_params']\n",
    "            param_names = self.param_names\n",
    "            mle_delta_chi2 = self.results['mle_delta_chi2']\n",
    "        \n",
    "        mle_modelmag = self._get_mag_func(mle_params, param_names, np.array(self.df['HJD-2450000']))\n",
    "        source_flux, blend_flux = self._calc_Fs_func(mle_modelmag, np.array(self.df['I_band_flux']), np.array(self.df['I_band_flux_err'])**2)\n",
    "        mle_model = source_flux * mle_modelmag + blend_flux\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        plt.errorbar(np.array(self.df['HJD-2450000']), np.array(self.df['I_band_flux']), yerr=np.array(self.df['I_band_flux_err']), \n",
    "                     fmt='o', color='black', label='Data', markersize=2, alpha=0.7)\n",
    "        plt.plot(np.array(self.df['HJD-2450000']), mle_model, color='red', linewidth=2, \n",
    "                 label=f'MCMC Best Fit - $\\\\Delta \\\\chi^2 = {mle_delta_chi2:.2f}$')\n",
    "        plt.xlabel('HJD - 2450000', fontsize=12)\n",
    "        plt.ylabel('Flux', fontsize=12)\n",
    "        plt.title('Binary Lens Model - MCMC Fit', fontsize=14)\n",
    "        plt.legend(loc='upper left', framealpha=0.8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_mcmc_traces(sampler, param_names, burn_in):\n",
    "        \"\"\"\n",
    "        Plot MCMC trace plots for convergence diagnosis\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sampler : emcee.EnsembleSampler\n",
    "            The MCMC sampler\n",
    "        param_names : list\n",
    "            Parameter names\n",
    "        burn_in : int\n",
    "            Burn-in period\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The trace plot figure\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(len(param_names), 1, figsize=(12, 2*len(param_names)), sharex=True)\n",
    "        if len(param_names) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i in range(len(param_names)):\n",
    "            # Plot all walker chains\n",
    "            for j in range(sampler.chain.shape[0]):\n",
    "                axes[i].plot(sampler.chain[j, :, i], color='k', alpha=0.4, linewidth=0.5)\n",
    "            \n",
    "            # Highlight a few random walkers for clarity\n",
    "            random_walkers = np.random.choice(sampler.chain.shape[0], size=min(5, sampler.chain.shape[0]), replace=False)\n",
    "            colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "            for idx, walker in enumerate(random_walkers):\n",
    "                color = colors[idx % len(colors)]\n",
    "                axes[i].plot(sampler.chain[walker, :, i], color=color, alpha=0.7, linewidth=1.2, \n",
    "                            label=f'Walker {walker}' if i == 0 else '')\n",
    "            \n",
    "            axes[i].axvline(burn_in, color='red', linestyle='--', linewidth=2, label='Burn-in' if i == 0 else '')\n",
    "            axes[i].set_ylabel(f'{param_names[i]}', fontsize=12)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            if i == 0:\n",
    "                axes[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        axes[-1].set_xlabel('Step Number', fontsize=12)\n",
    "        plt.suptitle('MCMC Trace Plots - Individual Walker Chains', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_convergence_diagnostics(sampler, param_names, burn_in):\n",
    "        \"\"\"\n",
    "        Plot running statistics to check convergence\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sampler : emcee.EnsembleSampler\n",
    "            The MCMC sampler\n",
    "        param_names : list\n",
    "            Parameter names\n",
    "        burn_in : int\n",
    "            Burn-in period\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The convergence diagnostics figure\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(len(param_names), 2, figsize=(16, 2*len(param_names)))\n",
    "        if len(param_names) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i in range(len(param_names)):\n",
    "            # Calculate running mean and std\n",
    "            chain_flat = sampler.chain[:, :, i]\n",
    "            running_mean = np.zeros((chain_flat.shape[0], chain_flat.shape[1]))\n",
    "            running_std = np.zeros((chain_flat.shape[0], chain_flat.shape[1]))\n",
    "            \n",
    "            for walker in range(chain_flat.shape[0]):\n",
    "                for step in range(1, chain_flat.shape[1]):\n",
    "                    running_mean[walker, step] = np.mean(chain_flat[walker, :step+1])\n",
    "                    running_std[walker, step] = np.std(chain_flat[walker, :step+1])\n",
    "            \n",
    "            # Plot running mean\n",
    "            step_numbers = np.arange(chain_flat.shape[1])\n",
    "            for walker in range(min(10, chain_flat.shape[0])):\n",
    "                axes[i, 0].plot(step_numbers, running_mean[walker, :], alpha=0.6, linewidth=1)\n",
    "            \n",
    "            axes[i, 0].axvline(burn_in, color='red', linestyle='--', linewidth=2, label='Burn-in')\n",
    "            axes[i, 0].set_ylabel(f'Running Mean - {param_names[i]}', fontsize=12)\n",
    "            axes[i, 0].set_title(f'{param_names[i]} - Running Mean Convergence', fontsize=12)\n",
    "            axes[i, 0].grid(True, alpha=0.3)\n",
    "            if i == 0:\n",
    "                axes[i, 0].legend()\n",
    "            \n",
    "            # Plot running standard deviation\n",
    "            for walker in range(min(10, chain_flat.shape[0])):\n",
    "                axes[i, 1].plot(step_numbers, running_std[walker, :], alpha=0.6, linewidth=1)\n",
    "            \n",
    "            axes[i, 1].axvline(burn_in, color='red', linestyle='--', linewidth=2, label='Burn-in')\n",
    "            axes[i, 1].set_ylabel(f'Running Std - {param_names[i]}', fontsize=12)\n",
    "            axes[i, 1].set_title(f'{param_names[i]} - Running Std Convergence', fontsize=12)\n",
    "            axes[i, 1].grid(True, alpha=0.3)\n",
    "            if i == 0:\n",
    "                axes[i, 1].legend()\n",
    "        \n",
    "        axes[-1, 0].set_xlabel('Step Number', fontsize=12)\n",
    "        axes[-1, 1].set_xlabel('Step Number', fontsize=12)\n",
    "        plt.suptitle('MCMC Convergence Diagnostics - Running Statistics', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def perform_mcmc_analysis(self, steps=3000, walkers=50, \n",
    "                             step_scale=1e-4, param_scales=None,\n",
    "                             verbose=False, plot_corner=False, plot_fit=False, \n",
    "                             plot_traces=False, plot_convergence=False, n_threads=None):\n",
    "        \"\"\"\n",
    "        Perform complete MCMC analysis of binary lens model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int, optional\n",
    "            Number of MCMC steps (default: 3000)\n",
    "        walkers : int, optional\n",
    "            Number of walkers (default: 50)\n",
    "        step_scale : float, optional\n",
    "            Global step size scaling factor (default: 1e-4)\n",
    "        param_scales : dict[str, float] | np.ndarray | None, optional\n",
    "            Dict step sizes by param name (preferred) or array in p-order.\n",
    "        verbose : bool, optional\n",
    "            If True, print detailed diagnostics (default: False)\n",
    "        plot_corner : bool, optional\n",
    "            If True, show corner plot (default: False)\n",
    "        plot_fit : bool, optional\n",
    "            If True, show best fit model plot (default: False)\n",
    "        plot_traces : bool, optional\n",
    "            If True, show trace plots (default: False)\n",
    "        plot_convergence : bool, optional\n",
    "            If True, show convergence diagnostics (default: False)\n",
    "        n_threads : int | None, optional\n",
    "            Number of parallel threads to use for MCMC. If None, runs in serial mode.\n",
    "            Uses Python's multiprocessing Pool to parallelize log probability evaluations\n",
    "            across walkers. (default: None)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        results : dict\n",
    "            Dictionary containing sampler, samples, MLE parameters, statistics, etc.\n",
    "            Also stored in self.results\n",
    "        \"\"\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Running MCMC with binary lens model...\")\n",
    "            print(\"Initial parameters:\", dict(zip(self.param_names, self.x0)))\n",
    "        \n",
    "        # Create multiprocessing pool if n_threads is specified\n",
    "        pool = None\n",
    "        if n_threads is not None:\n",
    "            pool = Pool(processes=n_threads)\n",
    "            if verbose:\n",
    "                print(f\"Using {n_threads} parallel processes for MCMC\")\n",
    "        \n",
    "        try:\n",
    "            # Run MCMC\n",
    "            sampler, pos, prob, state, free_param_names = self.run_mcmc(\n",
    "                steps=steps, walkers=walkers,\n",
    "                step_scale=step_scale, param_scales=param_scales, \n",
    "                verbose=verbose, pool=pool)\n",
    "        finally:\n",
    "            # Clean up pool\n",
    "            if pool is not None:\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "         \n",
    "        # Calculate acceptance fraction\n",
    "        mean_acceptance = np.mean(sampler.acceptance_fraction)\n",
    "        if verbose:\n",
    "            print(f\"\\nMean acceptance fraction: {mean_acceptance:.3f}\")\n",
    "        \n",
    "        # Calculate autocorrelation time with error handling\n",
    "        try:\n",
    "            autocorr_times = sampler.get_autocorr_time()\n",
    "            if verbose:\n",
    "                print(f\"Autocorrelation times: {autocorr_times}\")\n",
    "                print(f\"Max autocorrelation time: {np.max(autocorr_times):.1f}\")\n",
    "                \n",
    "                chain_length = sampler.chain.shape[1]\n",
    "                steps_needed = int(50 * np.max(autocorr_times))\n",
    "                if chain_length < steps_needed:\n",
    "                    print(f\"WARNING: Chain may not be converged. Consider running {steps_needed} steps.\")\n",
    "                else:\n",
    "                    print(\"Chain length appears adequate for convergence.\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Cannot reliably calculate autocorrelation time: {e}\")\n",
    "                print(\"Using alternative convergence diagnostics...\")\n",
    "            autocorr_times = None\n",
    "        \n",
    "        # Determine burn-in period\n",
    "        if autocorr_times is not None:\n",
    "            burn_in = min(int(2 * np.max(autocorr_times)), int(0.5 * sampler.chain.shape[1]))\n",
    "        else:\n",
    "            burn_in = int(0.3 * sampler.chain.shape[1])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Using burn-in period: {burn_in} steps ({burn_in/sampler.chain.shape[1]*100:.1f}% of chain)\")\n",
    "        \n",
    "        samples = sampler.chain[:, burn_in:, :].reshape((-1, len(free_param_names)))\n",
    "        \n",
    "        # Calculate convergence diagnostics\n",
    "        post_burnin_chains = sampler.chain[:, burn_in:, :]\n",
    "        r_hat_values = self.gelman_rubin_statistic(post_burnin_chains)\n",
    "        eff_sizes = self.effective_sample_size(post_burnin_chains)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nGelman-Rubin R-hat statistics (should be < 1.1 for convergence):\")\n",
    "            for i, param in enumerate(free_param_names):\n",
    "                if not np.isnan(r_hat_values[i]):\n",
    "                    status = \"\u2713 Good\" if r_hat_values[i] < 1.1 else \"\u26a0 Poor\" if r_hat_values[i] < 1.2 else \"\u2717 Bad\"\n",
    "                    print(f\"{param}: {r_hat_values[i]:.4f} ({status})\")\n",
    "                else:\n",
    "                    print(f\"{param}: Unable to calculate\")\n",
    "            \n",
    "            print(f\"\\nEffective sample sizes:\")\n",
    "            for i, param in enumerate(free_param_names):\n",
    "                print(f\"{param}: {eff_sizes[i]:.0f} (out of {len(samples)} total samples)\")\n",
    "        \n",
    "        # Find MLE in free space and reconstruct full params\n",
    "        max_likelihood_idx = np.argmax(sampler.lnprobability[:, burn_in:].flatten())\n",
    "        mle_free = samples[max_likelihood_idx]\n",
    "        \n",
    "        # Reconstruct full parameter vector from free samples\n",
    "        x0_map = dict(zip(self.param_names, self.x0))\n",
    "        fixed_params_map = self.fixed_params or {}\n",
    "        full_map = {**x0_map, **fixed_params_map}\n",
    "        for j, name in enumerate(free_param_names):\n",
    "            full_map[name] = float(mle_free[j])\n",
    "        mle_params = np.array([full_map[name] for name in self.param_names])\n",
    "        mle_dict = dict(zip(self.param_names, mle_params))\n",
    "        \n",
    "        # Calculate percentiles on free params only\n",
    "        percentiles = np.percentile(samples, [16, 50, 84], axis=0)\n",
    "        medians_free = percentiles[1]\n",
    "        upper_errors_free = percentiles[2] - percentiles[1]\n",
    "        lower_errors_free = percentiles[1] - percentiles[0]\n",
    "        \n",
    "        # Calculate chi-squared for full MLE\n",
    "        mle_chi2 = self._chi2_func(mle_params, self.param_names, np.array(self.df['HJD-2450000']), \n",
    "                                    np.array(self.df['I_band_flux']), np.array(self.df['I_band_flux_err']))\n",
    "        dof_params = len(free_param_names)\n",
    "        mle_delta_chi2 = mle_chi2 - (len(self.df) - dof_params)\n",
    "        \n",
    "        # Print basic results (always shown)\n",
    "        print(\"\\nMaximum Likelihood Estimate (MLE) parameters (full):\")\n",
    "        for param, value in mle_dict.items():\n",
    "            print(f\"{param}: {value:.6f}\")\n",
    "        \n",
    "        print(\"\\nMedian values with 1\u03c3 uncertainties (free params only):\")\n",
    "        for i, param in enumerate(free_param_names):\n",
    "            print(f\"{param}: {medians_free[i]:.6f} +{upper_errors_free[i]:.6f} -{lower_errors_free[i]:.6f}\")\n",
    "        \n",
    "        print(f\"\\nMLE Chi-squared: {mle_chi2:.2f}\")\n",
    "        print(f\"MLE Reduced chi-squared: {mle_chi2/(len(self.df)-dof_params):.2f}\")\n",
    "        print(f\"MLE Delta chi-squared: {mle_delta_chi2:.2f}\")\n",
    "        \n",
    "        # Assessment and recommendations\n",
    "        convergence_issues = []\n",
    "        recommendations = []\n",
    "        \n",
    "        if mean_acceptance < 0.2:\n",
    "            convergence_issues.append(\"Low acceptance fraction\")\n",
    "            recommendations.append(\"- Consider decreasing step size in proposal distribution\")\n",
    "        elif mean_acceptance > 0.7:\n",
    "            convergence_issues.append(\"High acceptance fraction\")\n",
    "            recommendations.append(\"- Consider increasing step size in proposal distribution\")\n",
    "        \n",
    "        if not np.isnan(r_hat_values).all():\n",
    "            poor_rhat = np.sum((r_hat_values > 1.1) & ~np.isnan(r_hat_values))\n",
    "            if poor_rhat > 0:\n",
    "                convergence_issues.append(f\"{poor_rhat} parameters have R-hat > 1.1\")\n",
    "                recommendations.append(\"- Run longer chains for better mixing\")\n",
    "        \n",
    "        min_eff_size = np.min(eff_sizes)\n",
    "        if min_eff_size < 100:\n",
    "            convergence_issues.append(\"Low effective sample sizes\")\n",
    "            recommendations.append(\"- Run longer chains to increase effective sample size\")\n",
    "        \n",
    "        if autocorr_times is not None:\n",
    "            max_tau = np.max(autocorr_times)\n",
    "            chain_length = sampler.chain.shape[1]\n",
    "            if chain_length < 50 * max_tau:\n",
    "                convergence_issues.append(\"Chain too short relative to autocorrelation time\")\n",
    "                recommendations.append(f\"- Run at least {int(50 * max_tau)} steps for reliable results\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CONVERGENCE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if convergence_issues:\n",
    "            print(\"\u26a0 POTENTIAL CONVERGENCE ISSUES:\")\n",
    "            for issue in convergence_issues:\n",
    "                print(f\"  \u2022 {issue}\")\n",
    "            \n",
    "        \n",
    "            print(\"\\n\ud83d\udca1 RECOMMENDATIONS:\")\n",
    "            for rec in recommendations:\n",
    "                print(f\"  {rec}\")\n",
    "        else:\n",
    "            print(\"\u2705 MCMC appears to have converged well!\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca SUMMARY:\")\n",
    "        print(f\"  \u2022 Total steps: {sampler.chain.shape[1]}\")\n",
    "        print(f\"  \u2022 Burn-in: {burn_in} steps\")\n",
    "        print(f\"  \u2022 Effective samples: {len(samples)}\")\n",
    "        print(f\"  \u2022 Acceptance rate: {mean_acceptance:.3f}\")\n",
    "        if autocorr_times is not None:\n",
    "            print(f\"  \u2022 Max autocorr time: {np.max(autocorr_times):.1f}\")\n",
    "        print(f\"  \u2022 Min effective size: {min_eff_size:.0f}\")\n",
    "        if not np.isnan(r_hat_values).all():\n",
    "            print(f\"  \u2022 Max R-hat: {np.nanmax(r_hat_values):.3f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        # Generate plots if requested\n",
    "        if plot_corner:\n",
    "            self.plot_corner_mcmc(samples, free_param_names)\n",
    "        \n",
    "        if plot_fit:\n",
    "            self.plot_mcmc_fit(mle_params, self.param_names, mle_delta_chi2)\n",
    "        \n",
    "        if plot_traces:\n",
    "            self.plot_mcmc_traces(sampler, free_param_names, burn_in)\n",
    "        \n",
    "        if plot_convergence:\n",
    "            self.plot_convergence_diagnostics(sampler, free_param_names, burn_in)\n",
    "        \n",
    "        # Build results dictionary\n",
    "        results = {\n",
    "            'sampler': sampler,\n",
    "            'samples': samples,  # Free parameter samples only\n",
    "            'free_param_names': free_param_names,\n",
    "            'fixed_params': fixed_params_map,\n",
    "            'mle_params': mle_params,  # Full parameter vector\n",
    "            'mle_dict': mle_dict,  # Full parameter dict\n",
    "            'medians': medians_free,  # Free parameter medians\n",
    "            'upper_errors': upper_errors_free,  # Free parameter upper errors\n",
    "            'lower_errors': lower_errors_free,  # Free parameter lower errors\n",
    "            'mle_chi2': mle_chi2,\n",
    "            'mle_delta_chi2': mle_delta_chi2,\n",
    "            'burn_in': burn_in,\n",
    "            'mean_acceptance': mean_acceptance,\n",
    "            'autocorr_times': autocorr_times,\n",
    "            'r_hat_values': r_hat_values,\n",
    "            'eff_sizes': eff_sizes,\n",
    "            'convergence_issues': convergence_issues,\n",
    "            'recommendations': recommendations\n",
    "        }\n",
    "        \n",
    "        # Store results as class attribute and return\n",
    "        self.results = results\n",
    "        return self.results\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
