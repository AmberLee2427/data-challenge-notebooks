{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f78e988",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Neural Posterior Estimation for Roman Microlensing\n",
    "\n",
    "***\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/rges-pit/data-challenge-notebooks/blob/main/Extras/nbi_roman_synthetic.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415cd71",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook you will have learned to:\n",
    "\n",
    "- Install and configure the **Neural Bayesian Inference (NBI)** package for microlensing analysis\n",
    "- Set up a **Point-Source Point-Lens (PSPL)** microlensing forward model\n",
    "- Configure **neural posterior estimation** with ResNet-GRU featurizers for time-series data\n",
    "- Train an **amortized neural posterior estimator** using simulated Roman microlensing data\n",
    "- Perform **rapid Bayesian inference** on synthetic light curves\n",
    "- Compare neural posterior estimation with traditional MCMC approaches\n",
    "\n",
    "In addition, you'll gain:\n",
    "- Understanding of simulation-based inference for microlensing\n",
    "- Experience with modern deep learning workflows for astronomical inference\n",
    "- Knowledge of when to use NBI versus traditional fitting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5f208",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use **Neural Bayesian Inference (NBI)** for rapid parameter estimation of Roman microlensing events. NBI is a simulation-based inference framework that uses neural networks to learn posterior distributions directly from simulated data.\n",
    "\n",
    "### Why Neural Posterior Estimation?\n",
    "\n",
    "Traditional Bayesian inference methods like MCMC are powerful but face fundamental scalability challenges:\n",
    "\n",
    "- **Computational cost**: Each MCMC run requires thousands of likelihood evaluations, taking hours to days per event\n",
    "- **Manual tuning**: Proposal distributions, burn-in periods, and convergence diagnostics require expert intervention\n",
    "- **Sequential nature**: Each event must be analyzed independently from scratch\n",
    "\n",
    "Neural Posterior Estimation (NPE) addresses these limitations through **amortized inference**—the computational cost is paid once during training, and inference on new events becomes nearly instantaneous.\n",
    "\n",
    "### Why Are We Doing This?\n",
    "\n",
    "Roman will discover tens of thousands of microlensing events. If each event takes 4 hours to fit with MCMC, processing 50,000 events would require **22 years of continuous compute time**. Even with parallelization, this presents a significant bottleneck.\n",
    "\n",
    "NPE offers a path to:\n",
    "- Process thousands of events in hours rather than years\n",
    "- Enable rapid triage to identify the most scientifically interesting events\n",
    "- Provide initial parameter estimates to seed more detailed analyses\n",
    "- Scale with Roman's data volume without proportional increases in compute\n",
    "\n",
    "### What's in It for You?\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand how normalizing flows learn to approximate posterior distributions\n",
    "- Be able to configure and train neural networks for microlensing inference\n",
    "- Know when NPE is appropriate versus traditional methods\n",
    "- Have hands-on experience with a complete simulation-based inference workflow\n",
    "- Be prepared to apply these techniques to Roman Data Challenge submissions\n",
    "\n",
    "### When to Use NPE vs Traditional Methods\n",
    "\n",
    "| Scenario | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| Thousands of events, initial characterization | NPE |\n",
    "| Single high-value event, publication-quality posteriors | MCMC |\n",
    "| Rapid triage to find planetary signals | NPE |\n",
    "| Complex models with many parameters | MCMC (more flexible) |\n",
    "| Real-time event classification | NPE |\n",
    "| Detailed systematics investigation | MCMC |\n",
    "\n",
    "The best workflows often combine both: use NPE for rapid initial estimates, then follow up with MCMC on the most interesting candidates.\n",
    "\n",
    "### Microlensing Parameter Refresher\n",
    "\n",
    "A point-source point-lens (PSPL) event is described by:\n",
    "\n",
    "- **$t_0$** – time of closest approach between the source and lens lines of sight\n",
    "- **$u_0$** – impact parameter in units of the angular Einstein radius ($\\theta_E$). Values $|u_0| \\ll 1$ yield high magnifications\n",
    "- **$t_E$** – Einstein timescale, the time for the lens-source separation to change by $\\theta_E$\n",
    "\n",
    "The instantaneous magnification for a PSPL event is:\n",
    "$$\n",
    "A(u) = \\frac{u^2 + 2}{u\\sqrt{u^2 + 4}}, \\qquad u^2 = u_0^2 + \\left(\\frac{t - t_0}{t_E}\\right)^2\n",
    "$$\n",
    "\n",
    "For a more detailed introduction to microlensing theory, refer to the learning resources on the [RGES-PIT website](https://rges-pit.org/resources/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08aea9f",
   "metadata": {},
   "source": [
    "### Notebook Contents\n",
    "\n",
    "The workflow for this notebook consists of:\n",
    "\n",
    "* [Installation & Setup](#1-installation--setup)\n",
    "* [Verify Installation](#2-verify-installation)\n",
    "* [Define Microlensing Model](#3-define-a-simple-microlensing-model)\n",
    "* [Prepare Data for NBI](#4-prepare-data-for-nbi)\n",
    "* [Visualize Simulated Light Curves](#5-visualize-simulated-light-curves)\n",
    "* [Set Up NBI Engine](#6-set-up-nbi-engine)\n",
    "  - [Normalizing Flow](#61-normalizing-flow)\n",
    "  - [Featurizer Network](#62-featurizer-network)\n",
    "* [Train the Model](#7-train-the-nbi-model-amortized-npe)\n",
    "* [Test Inference](#8-test-inference-on-synthetic-data)\n",
    "* [Analyze Results](#9-analyze-results)\n",
    "* [Summary](#10-summary)\n",
    "* [Additional Resources](#additional-resources)\n",
    "* [About this Notebook](#about-this-notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ef796",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "Before running this notebook, you need to install the NBI package and its dependencies.\n",
    "\n",
    "### Install NBI\n",
    "\n",
    "```bash\n",
    "pip install nbi\n",
    "```\n",
    "\n",
    "### Required Packages\n",
    "\n",
    "The following packages will be automatically installed:\n",
    "- `torch` - PyTorch for neural network training\n",
    "- `numpy` - Numerical computations\n",
    "- `matplotlib` - Plotting and visualization\n",
    "- `scipy` - Scientific computing utilities\n",
    "\n",
    "### Import Required Libraries\n",
    "\n",
    "Let's start by importing all necessary packages and verifying the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44daba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NBI and dependencies (run this cell if packages are not already installed)\n",
    "# Uncomment the line below if you are running on Colab or in an environment without NBI pre-installed\n",
    "# %pip install --quiet nbi torch numpy scipy matplotlib corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c1145",
   "metadata": {},
   "source": [
    "## 2. Verify Installation\n",
    "\n",
    "Let's verify that NBI is properly installed and check all required attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Import nbi with proper reload to ensure we get the installed version\n",
    "if 'nbi' in sys.modules:\n",
    "    import nbi\n",
    "    importlib.reload(nbi)\n",
    "else:\n",
    "    import nbi\n",
    "\n",
    "# Verify nbi is properly loaded\n",
    "print(f\"NBI version: {nbi.__version__}\")\n",
    "print(f\"NBI location: {nbi.__file__}\")\n",
    "print(f\"NBI.NBI class available: {hasattr(nbi, 'NBI')}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7faf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nbi installation and attributes\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Force reload of nbi to get the latest version\n",
    "if 'nbi' in sys.modules:\n",
    "    importlib.reload(sys.modules['nbi'])\n",
    "else:\n",
    "    import nbi\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"\\nnbi attributes:\")\n",
    "for attr in ['__version__', '__file__', 'NBI', 'get_featurizer']:\n",
    "    if hasattr(nbi, attr):\n",
    "        val = getattr(nbi, attr)\n",
    "        if callable(val):\n",
    "            print(f\"  [OK] {attr}: <callable>\")\n",
    "        else:\n",
    "            print(f\"  [OK] {attr}: {val}\")\n",
    "    else:\n",
    "        print(f\"  [MISSING] {attr}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nAll nbi attributes: {dir(nbi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c79aa9",
   "metadata": {},
   "source": [
    "### Installation Verified\n",
    "\n",
    "If you see `[OK]` markers above, the nbi package is correctly installed and imported.\n",
    "\n",
    "**Common Issues & Solutions:**\n",
    "- **\"module 'nbi' has no attribute 'NBI'\"**: Restart the kernel (Kernel -> Restart Kernel) and re-run cells\n",
    "- **\"No module named 'nbi'\"**: Make sure the correct kernel is selected in the top-right corner\n",
    "- **Import errors**: The cell above uses `importlib.reload()` to ensure the latest version is loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a15a6c",
   "metadata": {},
   "source": [
    "## 3. Define a Simple Microlensing Model\n",
    "\n",
    "We'll create a simplified point-source point-lens (PSPL) microlensing model for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pspl_model(t, params):\n",
    "    \"\"\"\n",
    "    Point Source Point Lens microlensing model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    t : array-like\n",
    "        Time array\n",
    "    params : tuple\n",
    "        (t0, u0, tE) where:\n",
    "        - t0: time of closest approach\n",
    "        - u0: impact parameter\n",
    "        - tE: Einstein crossing time\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    flux : array\n",
    "        Magnification as a function of time\n",
    "    \"\"\"\n",
    "    t0, u0, tE = params\n",
    "    \n",
    "    # Calculate angular separation u(t)\n",
    "    u = np.sqrt(u0**2 + ((t - t0) / tE)**2)\n",
    "    \n",
    "    # PSPL magnification\n",
    "    A = (u**2 + 2) / (u * np.sqrt(u**2 + 4))\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Test the model\n",
    "t_test = np.linspace(0, 100, 200)\n",
    "params_test = (50, 0.5, 20)  # t0, u0, tE\n",
    "\n",
    "flux_test = pspl_model(t_test, params_test)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(t_test, flux_test, 'b-', linewidth=2)\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Magnification')\n",
    "plt.title('Example PSPL Microlensing Light Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031683da",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for NBI\n",
    "\n",
    "We need to:\n",
    "1. Define the simulator function\n",
    "2. Define the noise function\n",
    "3. Set up the parameter priors\n",
    "4. Prepare the light curve data\n",
    "\n",
    "### Understanding Noise in Microlensing Light Curves\n",
    "\n",
    "Noise is a critical component for realistic training data. In real observations, photometric noise arises from:\n",
    "\n",
    "- **Photon noise (Poisson)**: Fundamental uncertainty from counting photons\n",
    "- **Sky background**: Varying atmospheric and instrumental background levels\n",
    "- **Systematic effects**: Detector artifacts, flat-fielding errors, etc.\n",
    "\n",
    "For simulation-based inference to generalize well to real data, the training noise model should approximate the noise properties of the target observations.\n",
    "\n",
    "### Our Noise Model\n",
    "\n",
    "We use a simple **heteroscedastic Gaussian noise model** where:\n",
    "- The noise level varies randomly between 0.01 and 0.1 (in magnification units)\n",
    "- Each simulation receives a different noise realization\n",
    "- This teaches the network to handle varying signal-to-noise ratios\n",
    "\n",
    "**Why variable noise?**\n",
    "Training with variable noise levels makes the network robust to different observing conditions. A network trained only on low-noise data will perform poorly on noisy observations, and vice versa.\n",
    "\n",
    "### Impact of Noise on Inference\n",
    "\n",
    "| Noise Level | Effect on Light Curve | Effect on Inference |\n",
    "|-------------|----------------------|---------------------|\n",
    "| Low (0.01)  | Clear peak and wings visible | Tight, accurate posteriors |\n",
    "| Medium (0.05) | Peak visible, wings noisy | Moderate uncertainties |\n",
    "| High (0.1+) | Peak may be obscured | Wide posteriors, possible biases |\n",
    "\n",
    "The network learns to output appropriately wide posteriors when the data is noisy, and tighter posteriors when the signal is strong - this is a key advantage of neural posterior estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "\n",
    "# Define time grid for light curves (normalized)\n",
    "t_grid = np.linspace(0, 200, 100)  # Simplified time grid\n",
    "\n",
    "# Simulator function for nbi\n",
    "def simulator(params):\n",
    "    \"\"\"\n",
    "    Wrapper for the PSPL model to work with nbi.\n",
    "    This function is self-contained to work with multiprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : tuple or array\n",
    "        (t0, u0, tE)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    array : simulated light curve\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Define time grid inside function for multiprocessing compatibility\n",
    "    t_grid = np.linspace(0, 200, 100)\n",
    "    \n",
    "    # Define PSPL model inside simulator for multiprocessing compatibility\n",
    "    def pspl_model(t, params):\n",
    "        \"\"\"Point Source Point Lens microlensing model.\"\"\"\n",
    "        t0, u0, tE = params\n",
    "        u = np.sqrt(u0**2 + ((t - t0) / tE)**2)\n",
    "        A = (u**2 + 2) / (u * np.sqrt(u**2 + 4))\n",
    "        return A\n",
    "    \n",
    "    return pspl_model(t_grid, params)\n",
    "\n",
    "# Noise function\n",
    "def noise_func(x, y):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to the simulated data.\n",
    "    \n",
    "    This function implements a heteroscedastic noise model where the noise\n",
    "    level varies randomly between simulations. This teaches the network to\n",
    "    handle varying signal-to-noise ratios and produce appropriately calibrated\n",
    "    uncertainties.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array\n",
    "        Simulated data (noise-free light curve)\n",
    "    y : array\n",
    "        Parameters (not used here but required by nbi API)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x_noisy : array \n",
    "        Light curve with added Gaussian noise\n",
    "    y : array\n",
    "        Unchanged parameters\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Noise level is drawn uniformly from [0.01, 0.1]\n",
    "    - Low noise (0.01): ~1% photometric precision, excellent conditions\n",
    "    - High noise (0.1): ~10% precision, challenging conditions\n",
    "    - Real Roman observations will have noise levels in this range\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Random noise level between 0.01 and 0.1 (1% to 10% photometric precision)\n",
    "    noise_level = np.random.uniform(0.01, 0.1)\n",
    "    \n",
    "    # Add Gaussian noise with the sampled noise level\n",
    "    noise = np.random.normal(0, noise_level, size=x.shape[0])\n",
    "    x_noisy = x + noise\n",
    "    \n",
    "    return x_noisy, y\n",
    "\n",
    "# Define priors based on the data statistics\n",
    "# Using reasonable ranges for microlensing events\n",
    "priors_dict = {\n",
    "    't0lens1': uniform(loc=50, scale=100),      # t0 in [50, 150]\n",
    "    'u0lens1': uniform(loc=0.0, scale=2.0),     # u0 in [0, 2]\n",
    "    'tE_helio': uniform(loc=5, scale=50)        # tE in [5, 55]\n",
    "}\n",
    "\n",
    "labels = list(priors_dict.keys())\n",
    "priors = [priors_dict[k] for k in labels]\n",
    "\n",
    "print(\"Defined priors:\")\n",
    "print(f\"  t0lens1: Uniform[{priors_dict['t0lens1'].kwds['loc']:.1f}, {priors_dict['t0lens1'].kwds['loc'] + priors_dict['t0lens1'].kwds['scale']:.1f}]\")\n",
    "print(f\"  u0lens1: Uniform[{priors_dict['u0lens1'].kwds['loc']:.1f}, {priors_dict['u0lens1'].kwds['loc'] + priors_dict['u0lens1'].kwds['scale']:.1f}]\")\n",
    "print(f\"  tE_helio: Uniform[{priors_dict['tE_helio'].kwds['loc']:.1f}, {priors_dict['tE_helio'].kwds['loc'] + priors_dict['tE_helio'].kwds['scale']:.1f}]\")\n",
    "    \n",
    "print(\"\\nNoise model:\")\n",
    "print(\"  Type: Heteroscedastic Gaussian\")\n",
    "print(\"  Noise level range: [0.01, 0.1] (1-10% photometric precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f14d2a",
   "metadata": {},
   "source": [
    "## 5. Visualize Simulated Light Curves\n",
    "\n",
    "Let's generate random light curves from the prior to verify our setup, and also demonstrate how different noise levels affect the observable signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18944bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Draw random parameters from prior\n",
    "    params_random = [p.rvs(1)[0] for p in priors]\n",
    "    \n",
    "    # Simulate light curve\n",
    "    lc_sim = simulator(params_random)\n",
    "    \n",
    "    # Add noise\n",
    "    lc_noisy, _ = noise_func(lc_sim, params_random)\n",
    "    \n",
    "    plt.plot(t_grid, lc_sim, 'b-', alpha=0.5, label='True')\n",
    "    plt.plot(t_grid, lc_noisy, 'r.', markersize=3, alpha=0.6, label='Noisy')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Magnification')\n",
    "    plt.title(f't0={params_random[0]:.1f}, u0={params_random[1]:.2f}, tE={params_random[2]:.1f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b77f39",
   "metadata": {},
   "source": [
    "### 5.1 Random Samples from the Prior\n",
    "\n",
    "First, let's visualize light curves with random parameters drawn from our prior distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of different noise levels on the same event\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Fixed parameters for comparison\n",
    "fixed_params = [100, 0.3, 25]  # t0, u0, tE\n",
    "lc_true = simulator(fixed_params)\n",
    "noise_levels = [0.01, 0.03, 0.07, 0.15]\n",
    "\n",
    "for ax, noise_lvl in zip(axes, noise_levels):\n",
    "    # Add noise at specific level\n",
    "    lc_noisy = lc_true + np.random.normal(0, noise_lvl, size=len(lc_true))\n",
    "    \n",
    "    ax.plot(t_grid, lc_true, 'b-', linewidth=2, alpha=0.7, label='True')\n",
    "    ax.errorbar(t_grid, lc_noisy, yerr=noise_lvl, fmt='r.', \n",
    "                markersize=4, alpha=0.6, label='Noisy')\n",
    "    ax.set_xlabel('Time (days)')\n",
    "    ax.set_ylabel('Magnification')\n",
    "    ax.set_title(f'Noise = {noise_lvl:.0%} precision')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Effect of Noise Level on Light Curve Quality', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Noise level affects both the visible signal quality and the resulting posterior widths.\")\n",
    "print(\"The network learns to produce wider posteriors for noisier data automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82579db",
   "metadata": {},
   "source": [
    "### 5.2 Effect of Noise Level on Light Curve Quality\n",
    "\n",
    "Now let's see how different noise levels affect the same underlying event. This demonstrates why training with variable noise is important - the network must learn to handle all these scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6d38e",
   "metadata": {},
   "source": [
    "## 6. Set Up NBI Engine\n",
    "\n",
    "The NBI engine consists of two main components that work together to learn posterior distributions:\n",
    "\n",
    "### 6.1 Normalizing Flow\n",
    "\n",
    "A **normalizing flow** is a type of generative model that learns to transform a simple base distribution (like a Gaussian) into a complex target distribution (our posterior). It works by:\n",
    "\n",
    "1. Starting with samples from a simple distribution (Mixture of Gaussians in our case)\n",
    "2. Applying a series of invertible transformations (flow blocks)\n",
    "3. Producing samples from the learned posterior distribution\n",
    "\n",
    "**Key parameters:**\n",
    "- `n_dims`: Number of parameters to infer (3 for t0, u0, tE)\n",
    "- `flow_hidden`: Hidden layer size controlling model capacity\n",
    "- `num_blocks`: Number of transformation layers (more blocks = more expressive)\n",
    "- `n_mog`: Number of Gaussian components in the base distribution\n",
    "\n",
    "### 6.2 Featurizer Network\n",
    "\n",
    "The **featurizer** is a neural network that compresses the input light curve into a fixed-size feature vector that the normalizing flow can condition on. For time-series data like light curves, we use a hybrid **ResNet-GRU** architecture:\n",
    "\n",
    "- **ResNet (Residual Network)**: Convolutional layers that extract local features from the light curve\n",
    "- **GRU (Gated Recurrent Unit)**: Recurrent layers that capture temporal dependencies\n",
    "\n",
    "**Key parameters:**\n",
    "- `type`: Architecture type ('resnet-gru' for sequential data)\n",
    "- `dim_in`: Number of input channels (1 for flux-only data)\n",
    "- `dim_out`: Output feature dimension (should match flow expectations)\n",
    "- `dim_conv_max`: Maximum hidden dimension in convolutional layers\n",
    "- `depth`: Number of ResNet blocks (more depth = more feature extraction capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the normalizing flow\n",
    "flow_config = {\n",
    "    'n_dims': 3,              # Number of parameters (t0, u0, tE)\n",
    "    'flow_hidden': 128,       # Hidden dimension for flow\n",
    "    'num_blocks': 5,          # Number of flow blocks\n",
    "    'n_mog': 4                # Number of Mixture of Gaussians as base density\n",
    "}\n",
    "\n",
    "# Configure the featurizer network (ResNet-GRU for sequential data)\n",
    "featurizer_config = {\n",
    "    'type': 'resnet-gru',     # Hybrid CNN-RNN architecture for light curves\n",
    "    'norm': 'weight_norm',    # Normalization method\n",
    "    'dim_in': 1,              # Number of input channels (just flux)\n",
    "    'dim_out': 128,           # Output feature dimension\n",
    "    'dim_conv_max': 256,      # Maximum hidden dimension for CNN\n",
    "    'depth': 3                # Number of ResNet blocks\n",
    "}\n",
    "\n",
    "print(\"Flow configuration:\")\n",
    "for k, v in flow_config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "    \n",
    "print(\"\\nFeaturizer configuration:\")\n",
    "for k, v in featurizer_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a03f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NBI engine\n",
    "engine = nbi.NBI(\n",
    "    flow=flow_config,\n",
    "    featurizer=featurizer_config.copy(), # Use copy to avoid key deletion on re-run\n",
    "    simulator=simulator,\n",
    "    priors=priors,\n",
    "    labels=labels,\n",
    "    device='cpu',              # Use 'cuda' if GPU is available\n",
    "    path='nbi_roman_synthetic',   # Directory to save checkpoints\n",
    "    n_jobs=2                  # Use 2 jobs for parallel processing\n",
    ")\n",
    "\n",
    "print(\"NBI engine initialized successfully!\")\n",
    "print(f\"  Device: {engine.device}\")\n",
    "print(f\"  Model ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a2b0c",
   "metadata": {},
   "source": [
    "## 7. Train the NBI Model (Amortized NPE)\n",
    "\n",
    "We'll train the model using **Amortized Neural Posterior Estimation (ANPE)** - a single round of training on samples drawn from the prior.\n",
    "\n",
    "### Understanding the Training Process\n",
    "\n",
    "**Training Data Generation:**\n",
    "- The simulator generates `n_sims` light curves with parameters drawn from the prior\n",
    "- Each light curve is paired with its true parameters\n",
    "- Noise is added to simulate realistic observations\n",
    "\n",
    "**Training Strategy:**\n",
    "- **Epochs**: The model trains for up to `n_epochs` iterations over the full dataset\n",
    "- **Batch size**: Controls how many samples are processed together (affects GPU memory and convergence)\n",
    "- **Early stopping**: Training stops if validation loss doesn't improve for `patience` epochs\n",
    "- **Training/Validation Split**: By default, 90% of simulations are used for training, 10% for validation\n",
    "\n",
    "**Why 90% Training Split?**\n",
    "The 90/10 split balances having enough training data for the neural network to learn complex posterior mappings while retaining sufficient validation data to detect overfitting. With 5000 simulations, this means 4500 training samples and 500 validation samples.\n",
    "\n",
    "**Note**: This is a demonstration with a small number of simulations. For production use, you would typically use 50,000+ simulations for better posterior coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5992b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "n_sims = 50000          # Number of simulations per round\n",
    "n_rounds = 1           # Number of training rounds (ANPE)\n",
    "n_epochs = 100         # Number of training epochs\n",
    "batch_size = 256       # Batch size\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "print(\"Training NBI model...\")\n",
    "print(f\"  Simulations: {n_sims}\")\n",
    "print(f\"  Rounds: {n_rounds}\")\n",
    "print(f\"  Epochs: {n_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(\"\\nThis may take a few minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ae200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Note: engine.fit() does not return a history object\n",
    "# The training/validation losses are stored in engine.train_losses and engine.val_losses\n",
    "engine.fit(\n",
    "    n_sims=n_sims,\n",
    "    n_rounds=n_rounds,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=learning_rate,\n",
    "    early_stop_patience=20,  # Stop if no improvement for 20 epochs\n",
    "    noise=noise_func, \n",
    "    workers=4                # Use 4 workers for parallel data loading\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525dbeb0",
   "metadata": {},
   "source": [
    "### Understanding the Training Output\n",
    "\n",
    "**What do \"Train Loglike in nats\" and \"Val Loglike in nats\" mean?**\n",
    "\n",
    "The training output shows the **log-likelihood** of the data under the learned posterior distribution, measured in **nats** (natural logarithm units, as opposed to bits which use log base 2).\n",
    "\n",
    "- **Log-likelihood**: Measures how well the normalizing flow models the true posterior distribution. Higher (less negative) values indicate better model fit.\n",
    "- **Nats**: The natural unit for information when using natural logarithm ($\\ln$). 1 nat ≈ 1.44 bits.\n",
    "- **Train Loglike**: Average log-likelihood on training data — indicates how well the model fits the training simulations.\n",
    "- **Val Loglike**: Average log-likelihood on validation data — indicates how well the model generalizes to unseen simulations.\n",
    "\n",
    "**Early Stopping Criterion:**\n",
    "\n",
    "Early stopping monitors the **validation log-likelihood**. Training halts when the validation log-likelihood hasn't improved for `early_stop_patience` consecutive epochs (20 in our case). This prevents overfitting by stopping before the model starts memorizing training data at the expense of generalization.\n",
    "\n",
    "**Interpreting the values:**\n",
    "- Values closer to 0 (less negative) = better model performance\n",
    "- If train loglike >> val loglike: possible overfitting\n",
    "- If both improve together: healthy training\n",
    "- Typical converged values depend on the problem complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1fc461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "# NBI (version 0.4.1) stores losses in engine.tloss and engine.vloss\n",
    "# These are simple lists containing per-epoch loss values (negative log-likelihoods)\n",
    "# We negate them to convert to positive log-likelihoods for plotting\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# Get losses - negate to convert from negative log-likelihood to log-likelihood\n",
    "train_losses = -np.array(engine.tloss)\n",
    "val_losses = -np.array(engine.vloss)\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "ax.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Log-likelihood', marker='o', markersize=3)\n",
    "ax.plot(epochs, val_losses, 'r-', linewidth=2, label='Validation Log-likelihood', marker='s', markersize=3)\n",
    "\n",
    "# Mark the best epoch (highest validation log-likelihood = lowest negative log-likelihood)\n",
    "best_epoch = np.argmax(val_losses) + 1\n",
    "best_val = np.max(val_losses)\n",
    "ax.axvline(best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "ax.scatter([best_epoch], [best_val], color='green', s=100, zorder=5, marker='*')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Log-likelihood (nats)', fontsize=12)\n",
    "ax.set_title('Training Progress: Log-likelihood vs Epoch', fontsize=14)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation for best epoch (with safe positioning)\n",
    "x_offset = max(1, len(epochs) * 0.1)\n",
    "y_range = np.max(val_losses) - np.min(val_losses)\n",
    "y_offset = y_range * 0.1 if y_range > 0 else 0.5\n",
    "ax.annotate(f'Best: {best_val:.2f} nats', \n",
    "            xy=(best_epoch, best_val), \n",
    "            xytext=(min(best_epoch + x_offset, len(epochs)), best_val - y_offset),\n",
    "            fontsize=10, \n",
    "            arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Total epochs trained: {len(train_losses)}\")\n",
    "print(f\"  Best epoch: {best_epoch}\")\n",
    "print(f\"  Best validation log-likelihood: {best_val:.4f} nats\")\n",
    "print(f\"  Final training log-likelihood: {train_losses[-1]:.4f} nats\")\n",
    "print(f\"  Final validation log-likelihood: {val_losses[-1]:.4f} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f74e55",
   "metadata": {},
   "source": [
    "### Save and Load the Best Model Checkpoint\n",
    "\n",
    "During training, NBI automatically saves the model checkpoint with the **best validation log-likelihood**. This ensures that even if training continues past the optimal point (before early stopping triggers), you always have access to the best-performing model.\n",
    "\n",
    "**Why save checkpoints?**\n",
    "- **Avoid retraining**: Training can take hours — saving allows you to reuse the trained model\n",
    "- **Best model selection**: The checkpoint corresponds to the epoch with highest validation performance\n",
    "- **Reproducibility**: Share trained models with collaborators or use across different sessions\n",
    "- **Deployment**: Load the trained model for rapid inference on new events\n",
    "\n",
    "**Checkpoint contents:**\n",
    "- Neural network weights (featurizer + normalizing flow)\n",
    "- Optimizer state (for resuming training if needed)\n",
    "- Training configuration metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6da35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display saved model information and demonstrate how to reload\n",
    "import os\n",
    "\n",
    "print(\"Trained Model Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NBI saves checkpoints in: {path}/{round}/{epoch}.pth\n",
    "# Find the best epoch from validation losses\n",
    "output_dir = engine.directory  # Use engine's directory attribute\n",
    "best_epoch_idx = np.argmin(engine.vloss)  # Index of best epoch (lowest loss = best)\n",
    "round_dir = os.path.join(output_dir, str(engine.round))\n",
    "best_checkpoint = os.path.join(round_dir, f\"{best_epoch_idx}.pth\")\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Best epoch: {best_epoch_idx}\")\n",
    "print(f\"Best checkpoint path: {best_checkpoint}\")\n",
    "\n",
    "if os.path.exists(best_checkpoint):\n",
    "    size_kb = os.path.getsize(best_checkpoint) / 1024\n",
    "    print(f\"Checkpoint exists: True ({size_kb:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"Checkpoint exists: False\")\n",
    "\n",
    "# List all saved files in the output directory\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"\\nFiles saved in '{output_dir}/' directory:\")\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        for f in sorted(files):\n",
    "            fpath = os.path.join(root, f)\n",
    "            size_kb = os.path.getsize(fpath) / 1024\n",
    "            rel_path = os.path.relpath(fpath, output_dir)\n",
    "            print(f\"  - {rel_path} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"To load this trained model in a NEW session:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "# Step 1: Redefine the simulator and priors (must match training)\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "\n",
    "def simulator(params):\n",
    "    t_grid = np.linspace(0, 200, 100)\n",
    "    t0, u0, tE = params\n",
    "    u = np.sqrt(u0**2 + ((t_grid - t0) / tE)**2)\n",
    "    A = (u**2 + 2) / (u * np.sqrt(u**2 + 4))\n",
    "    return A\n",
    "\n",
    "priors = [\n",
    "    uniform(loc=50, scale=100),   # t0 in [50, 150]\n",
    "    uniform(loc=0.0, scale=2.0),  # u0 in [0, 2]\n",
    "    uniform(loc=5, scale=50)      # tE in [5, 55]\n",
    "]\n",
    "labels = ['t0lens1', 'u0lens1', 'tE_helio']\n",
    "\n",
    "# Step 2: Load the trained model\n",
    "import nbi\n",
    "\n",
    "# Load checkpoint and scaling parameters\n",
    "checkpoint_path = '{best_checkpoint}'\n",
    "scales_dir = '{round_dir}'\n",
    "\n",
    "engine = nbi.NBI(\n",
    "    simulator=simulator,\n",
    "    priors=priors,\n",
    "    labels=labels,\n",
    "    device='cuda',  # or 'cpu'\n",
    "    path='{output_dir}'  # Same path used during training\n",
    ")\n",
    "\n",
    "# Load the saved network weights\n",
    "import torch\n",
    "engine.get_network().load_state_dict(\n",
    "    torch.load(checkpoint_path, map_location=engine.device)\n",
    ")\n",
    "\n",
    "# Load scaling parameters if saved\n",
    "x_scales = np.load(os.path.join(scales_dir, 'x_scales.npy'))\n",
    "y_scales = np.load(os.path.join(scales_dir, 'y_scales.npy'))\n",
    "engine.x_mean, engine.x_std = x_scales\n",
    "engine.y_mean, engine.y_std = y_scales\n",
    "\n",
    "# Step 3: Run inference on new data\n",
    "# new_lightcurve = ...  # Your observed light curve\n",
    "# errors = ...          # Photometric uncertainties\n",
    "# y_pred, weights = engine.predict(new_lightcurve, x_err=errors, n_samples=5000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25b10b",
   "metadata": {},
   "source": [
    "## 8. Test Inference on Synthetic Data\n",
    "\n",
    "Now we test the trained model by performing inference on synthetic light curves. This demonstrates the key advantage of amortized inference: once trained, inference takes only seconds per event.\n",
    "\n",
    "### How Inference Works\n",
    "\n",
    "1. **Input**: A light curve (flux values + uncertainties)\n",
    "2. **Featurization**: The ResNet-GRU network compresses the light curve into a feature vector\n",
    "3. **Posterior Sampling**: The normalizing flow generates samples from the learned posterior, conditioned on the features\n",
    "4. **Importance Reweighting** (optional): Samples can be reweighted using the prior to improve posterior estimates\n",
    "\n",
    "### Testing on Multiple Events\n",
    "\n",
    "To properly evaluate the trained model, we'll test on several synthetic events with different parameter combinations. This helps us understand:\n",
    "- How well the model recovers true parameters across the prior range\n",
    "- Whether there are regions of parameter space where inference is less reliable\n",
    "- The typical uncertainty levels for different event types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f899da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple test events with different parameter combinations\n",
    "test_events = [\n",
    "    {'params': [100, 0.3, 25], 'name': 'High-mag, short tE'},\n",
    "    {'params': [80, 0.8, 35], 'name': 'Low-mag, medium tE'},\n",
    "    {'params': [120, 0.1, 15], 'name': 'Very high-mag, short tE'},\n",
    "    {'params': [90, 1.2, 45], 'name': 'Weak event, long tE'},\n",
    "]\n",
    "\n",
    "# Generate test light curves for all events\n",
    "np.random.seed(123)\n",
    "noise_level = 0.05\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "test_data = []\n",
    "for i, event in enumerate(test_events):\n",
    "    params = event['params']\n",
    "    \n",
    "    # Simulate the light curve\n",
    "    lc_test = simulator(params)\n",
    "    \n",
    "    # Add noise\n",
    "    lc_obs = lc_test + np.random.normal(0, noise_level, size=len(lc_test))\n",
    "    lc_err = np.ones_like(lc_obs) * noise_level\n",
    "    \n",
    "    # Store for later inference\n",
    "    test_data.append({\n",
    "        'params': params,\n",
    "        'name': event['name'],\n",
    "        'lc_obs': lc_obs,\n",
    "        'lc_err': lc_err,\n",
    "        'lc_true': lc_test\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    ax.plot(t_grid, lc_test, 'b-', linewidth=2, label='True Model', alpha=0.7)\n",
    "    ax.errorbar(t_grid, lc_obs, yerr=lc_err, fmt='r.', alpha=0.5, \n",
    "                markersize=3, label='Observed Data')\n",
    "    ax.set_xlabel('Time (days)')\n",
    "    ax.set_ylabel('Magnification')\n",
    "    ax.set_title(f\"Event {i+1}: {event['name']}\\nt0={params[0]}, u0={params[1]}, tE={params[2]}\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(test_events)} test events for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on all test events\n",
    "n_samples = 50000  # Number of posterior samples per event\n",
    "\n",
    "all_results = []\n",
    "for i, event in enumerate(test_data):\n",
    "    print(f\"\\nPerforming inference on Event {i+1}: {event['name']}...\")\n",
    "    \n",
    "    y_pred, weights = engine.predict(\n",
    "        event['lc_obs'], \n",
    "        x_err=event['lc_err'],\n",
    "        y_true=event['params'],\n",
    "        n_samples=n_samples,\n",
    "        corner=True  # Skip corner plots for batch processing\n",
    "    )\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    results = {'name': event['name'], 'params': event['params']}\n",
    "    for j, label in enumerate(labels):\n",
    "        mean = np.average(y_pred[:, j], weights=weights)\n",
    "        std = np.sqrt(np.average((y_pred[:, j] - mean)**2, weights=weights))\n",
    "        results[f'{label}_mean'] = mean\n",
    "        results[f'{label}_std'] = std\n",
    "        results[f'{label}_true'] = event['params'][j]\n",
    "    \n",
    "    results['y_pred'] = y_pred\n",
    "    results['weights'] = weights\n",
    "    all_results.append(results)\n",
    "\n",
    "print(f\"\\nInference completed for all {len(test_data)} events!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e4ae09",
   "metadata": {},
   "source": [
    "## 9. Analyze Results\n",
    "\n",
    "Let's examine the posterior distributions for all test events and compare the inferred parameters with true values. This analysis helps us understand:\n",
    "\n",
    "- **Accuracy**: How close are the posterior means to the true values?\n",
    "- **Precision**: How tight are the posterior distributions?\n",
    "- **Calibration**: Do the 68% credible intervals contain the true values ~68% of the time?\n",
    "\n",
    "### Parameter Recovery Summary\n",
    "\n",
    "We'll first create a summary table showing recovery statistics for all events, then visualize the 1D marginalized posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfa052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of parameter recovery across all events\n",
    "print(\"Parameter Recovery Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Event':<25} {'Parameter':<12} {'True':>10} {'Inferred':>15} {'Error':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for result in all_results:\n",
    "    for j, label in enumerate(labels):\n",
    "        true_val = result[f'{label}_true']\n",
    "        mean_val = result[f'{label}_mean']\n",
    "        std_val = result[f'{label}_std']\n",
    "        error = abs(mean_val - true_val) / true_val * 100\n",
    "        \n",
    "        event_name = result['name'] if j == 0 else \"\"\n",
    "        print(f\"{event_name:<25} {label:<12} {true_val:>10.2f} {mean_val:>8.2f} +/- {std_val:<5.2f} {error:>8.1f}%\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892761cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1D marginalized posteriors for all events\n",
    "fig, axes = plt.subplots(len(test_data), 3, figsize=(15, 4*len(test_data)))\n",
    "\n",
    "for row, result in enumerate(all_results):\n",
    "    y_pred = result['y_pred']\n",
    "    weights = result['weights']\n",
    "    true_params = result['params']\n",
    "    \n",
    "    for col, label in enumerate(labels):\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Weighted histogram\n",
    "        ax.hist(y_pred[:, col], bins=30, weights=weights, alpha=0.7, \n",
    "                edgecolor='black', density=True, color='steelblue')\n",
    "        \n",
    "        # True value\n",
    "        ax.axvline(true_params[col], color='red', linestyle='--', \n",
    "                   linewidth=2, label='True Value')\n",
    "        \n",
    "        # Posterior median and uncertainties\n",
    "        sorted_idx = np.argsort(y_pred[:, col])\n",
    "        cumsum = np.cumsum(weights[sorted_idx])\n",
    "        cumsum /= cumsum[-1]\n",
    "        \n",
    "        p16 = y_pred[sorted_idx[np.searchsorted(cumsum, 0.16)], col]\n",
    "        p50 = y_pred[sorted_idx[np.searchsorted(cumsum, 0.50)], col]\n",
    "        p84 = y_pred[sorted_idx[np.searchsorted(cumsum, 0.84)], col]\n",
    "        \n",
    "        ax.axvline(p50, color='navy', linestyle='-', linewidth=2, label='Median')\n",
    "        ax.axvspan(p16, p84, alpha=0.2, color='navy', label='68% CI')\n",
    "        \n",
    "        ax.set_xlabel(label)\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(f\"Event {row+1}\\nProbability Density\")\n",
    "        else:\n",
    "            ax.set_ylabel('Probability Density')\n",
    "        \n",
    "        if row == 0:\n",
    "            ax.set_title(f'{label} Posterior')\n",
    "        if row == 0 and col == 2:\n",
    "            ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12963cc",
   "metadata": {},
   "source": [
    "### Understanding Parameter Constraints\n",
    "\n",
    "Different microlensing parameters have different levels of constraining power:\n",
    "\n",
    "**Impact Parameter (u0):**\n",
    "- Low u0 (high magnification): Strongly constrained - the peak magnification directly encodes u0\n",
    "- High u0 (weak events): Less constrained - the light curve shape becomes more degenerate\n",
    "\n",
    "**Einstein Crossing Time (tE):**\n",
    "- Controlled by the width of the magnification curve\n",
    "- Well-constrained for high-magnification events with good sampling\n",
    "- May be degenerate with u0 for sparsely sampled data\n",
    "\n",
    "**Time of Peak (t0):**\n",
    "- Usually the best-constrained parameter\n",
    "- Directly observable as the time of maximum magnification\n",
    "- Precision depends on sampling density near the peak\n",
    "\n",
    "### How Noise Affects Inference Quality\n",
    "\n",
    "The noise level in the observed light curve directly impacts posterior precision:\n",
    "\n",
    "| Noise Level | Signal-to-Noise | t0 Precision | u0 Precision | tE Precision |\n",
    "|-------------|-----------------|--------------|--------------|--------------|\n",
    "| 1% (0.01)   | Excellent       | < 0.5 days   | < 5%         | < 5%         |\n",
    "| 5% (0.05)   | Good            | ~1-2 days    | ~10-20%      | ~10-15%      |\n",
    "| 10% (0.10)  | Moderate        | ~2-5 days    | ~20-50%      | ~15-30%      |\n",
    "| >15%        | Poor            | May be biased| Large uncertainty | Large uncertainty |\n",
    "\n",
    "**Key insight**: For weak events (high u0) combined with high noise, the posterior may become prior-dominated, meaning the data provides little constraining power. The network correctly learns to output wider posteriors in these cases.\n",
    "\n",
    "### Effect of Model Parameters on Inference Quality\n",
    "\n",
    "| Parameter | Increase Effect | Decrease Effect |\n",
    "|-----------|-----------------|-----------------|\n",
    "| n_sims | Better posterior coverage | Faster training, less accurate |\n",
    "| n_epochs | Better convergence | Underfitting |\n",
    "| flow_hidden | More expressive posteriors | Faster, less complex |\n",
    "| num_blocks | More flexible density estimation | Simpler, faster |\n",
    "| depth (featurizer) | Better feature extraction | Lighter model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9143f80d",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow for using Neural Bayesian Inference (NBI) with Roman microlensing data:\n",
    "\n",
    "1. **Setup**: Installed and configured the NBI package with required dependencies\n",
    "2. **Forward Model**: Implemented a Point-Source Point-Lens (PSPL) microlensing simulator\n",
    "3. **Architecture**: Configured normalizing flows for posterior estimation and ResNet-GRU featurizers for light curve encoding\n",
    "4. **Training**: Trained an amortized neural posterior estimator on 5000 synthetic simulations\n",
    "5. **Inference**: Performed rapid parameter inference on multiple test events\n",
    "6. **Analysis**: Evaluated posterior distributions and parameter recovery statistics\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Amortized inference** enables rapid analysis of many events after a one-time training cost\n",
    "- **Normalizing flows** can learn complex, multi-modal posterior distributions\n",
    "- **Deep featurizers** automatically extract relevant features from light curves\n",
    "- **Parameter constraints** vary based on event properties (u0, tE, sampling)\n",
    "\n",
    "### Recommendations for Further Use\n",
    "\n",
    "- Consider Sequential NPE (`n_rounds > 1`) for challenging events\n",
    "- Validate on held-out real data before deployment\n",
    "- Include additional physics (parallax, finite source) as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257ee1",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### NBI and Simulation-Based Inference\n",
    "- [NBI Documentation](https://nbi.readthedocs.io/) — API details, examples, and configuration options.\n",
    "- [NBI GitHub Repository](https://github.com/kmzzhang/nbi/tree/main#) — source code and issue tracker.\n",
    "- [SBI Library](https://www.mackelab.org/sbi/) — alternative simulation-based inference toolkit with similar capabilities.\n",
    "- Papamakarios et al. 2021, \"Normalizing Flows for Probabilistic Modeling and Inference\" — comprehensive review of normalizing flow methods.\n",
    "\n",
    "### Microlensing Resources\n",
    "- [MulensModel documentation](https://rpoleski.github.io/MulensModel/) — API details, geometry references, and Roman-specific notes.\n",
    "- [Roman Research Nexus](https://roman.ipac.caltech.edu/nexus) — information about kernels, data access, and team spaces.\n",
    "- [RGES-PIT Microlensing resources](https://rges-pit.org/outreach/) — minicourse videos and supplementary tutorials.\n",
    "- [microlens-submit docs](https://microlens-submit.readthedocs.io/en/latest/) — workflow for packaging and validating challenge submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b60669",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "**Author:** Meet J. Vyas  \n",
    "**Maintainers:** RGES-PIT Working Group 9  \n",
    "**Last Updated:** December 2025  \n",
    "**Contact:** For questions or contributions, please open an issue on the [data-challenge-notebooks repository](https://github.com/rges-pit/data-challenge-notebooks)\n",
    "\n",
    "## Source\n",
    "\n",
    "This notebook is maintained in the [data-challenge-notebooks repository](https://github.com/rges-pit/data-challenge-notebooks).\n",
    "\n",
    "## Citations\n",
    "\n",
    "If you use `nbi`, or this notebook for published research, please cite the authors:\n",
    "\n",
    "### Citing NBI\n",
    "* Zhang et al. 2023, \"Neural Posterior Estimation for Exoplanetary Atmospheres\" (NeurIPS Workshop on Machine Learning and the Physical Sciences)\n",
    "* [NBI GitHub Repository](https://github.com/kmzzhang/nbi/tree/main#)\n",
    "\n",
    "### Citing PyTorch\n",
    "* Paszke et al. 2019, \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\"\n",
    "* [PyTorch](https://pytorch.org/)\n",
    "\n",
    "### Citing Roman Microlensing Data Challenge 2026 Notebooks\n",
    "\n",
    "If you use our notebooks in your project, please cite:\n",
    "\n",
    "```\n",
    "Malpas, A., Murlidhar, A., Vandorou, K., Kruszyńska, K., Crisp, A., & Vyas, M. (2026). Roman Microlensing Data Challenge 2026 Notebooks (v1.0.0). Zenodo. https://doi.org/10.5281/zenodo.18262183\n",
    "```\n",
    "\n",
    "**BibTeX:**\n",
    "```bibtex\n",
    "@software{malpas_2025_17806271,\n",
    "  author       = {Malpas, Amber and Murlidhar, Arjun and Vyas, Meet and Vandorou, Katie and Kruszyńska, Katarzyna and Crisp, Ali},\n",
    "  title        = {Roman Microlensing Data Challenge 2026 Notebooks},\n",
    "  month        = dec,\n",
    "  year         = 2026,\n",
    "  publisher    = {Zenodo},\n",
    "  version      = {v1.0.0},\n",
    "  doi          = {10.5281/zenodo.18262183},\n",
    "  url          = {https://doi.org/10.5281/zenodo.18262183}\n",
    "}\n",
    "```\n",
    "\n",
    "[Top of Page](#top)\n",
    "<!-- Footer Start -->\n",
    "\n",
    "[Top of Page](#top)\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "\t<img src=\"https://raw.githubusercontent.com/rges-pit/data-challenge-notebooks/refs/heads/main/rges-pit_logo.png\" alt=\"RGES-PIT Logo\" width=\"150\"/>\n",
    "\t<img style=\"margin-top: 40px;\" src=\"https://raw.githubusercontent.com/spacetelescope/roman_notebooks/refs/heads/main/stsci_logo2.png\" alt=\"Space Telescope Logo\" width=\"200px\"/>\n",
    "</div>\n",
    "\n",
    "<!-- Footer End -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rges_sync": {
   "source_id": "nbi-roman-simulations",
   "session": "Extras",
   "audiences": [
    "Data challenge teams",
    "Self-paced learners"
   ],
   "website_render": "full_embed",
   "nexus_support": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
